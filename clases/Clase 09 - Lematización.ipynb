{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 9\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "## Lematización\n",
    "La implementación de similitud de palabras, puede ser un poco alejada de la realidad, ya que aunque nuestros tokens se encuentran limpios de caractéres innecesarios, palabras como *dije*, *dijiste*, *diremos* o *diré*, son tomadas como diferentes tokens en nuestra implementación de la Practica 1, y todas estas palabras pertencesn al **lema** (La raíz léxica de una palabra) *decir*, por lo que tener un contexto para una palabra como este:\n",
    "```json\n",
    "{\n",
    "    \"word\":[\n",
    "        \"dije\":1,\n",
    "        \"dijiste\":1,\n",
    "        \"diremos\":1,\n",
    "        \"diré\":1\n",
    "        ]\n",
    "}\n",
    "```\n",
    "\n",
    "Debería verse de esta forma:\n",
    "```json\n",
    "{\n",
    "    \"word\":[\n",
    "        \"decir\",4\n",
    "        ]\n",
    "}\n",
    "```\n",
    "Esto es debido a que hablamos del mismo concepto. La **lematización** (O *lemmatization* en inglés) es el proceso *correcto* a realizar como parte de la normalización del texto, y está basado en vocabulario, mofología y raíces léxicas así como el avance evolutivo del lenguaje. Sin embargo es un proceso es muy complejo computacionalmente, y aún más si se requieres realizar la inferecia de los lemas. Por tanto, existen recursos que nos proveen de mapas de lemas con palabras flexadas (La modificaciones que se le hacen a los lemas para crear conjugaciones, etc..), esto nos ayudará en nuestro proceso de lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(txt):\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\treturn BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_trash(txt):\n",
    "\timport re\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in txt:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(txt):\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tans = []\n",
    "\tstp = stopwords.words()\n",
    "\tfor w in txt:\n",
    "\t\tif w not in stp:\n",
    "\t\t\tans.append(w)\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokens):\n",
    "\tvocabulary = sorted(set(tokens))\n",
    "\treturn vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(wordList, windowSize=4):\n",
    "    contexts = dict()\n",
    "    index = 0\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if word not in contexts:\n",
    "            contexts[word] = []\n",
    "        start = max(0, index - windowSize)\n",
    "        end = min(index + windowSize, len(wordList) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i != index:\n",
    "                contexts[word].append(wordList[i])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpToJson(obj, name):\n",
    "    import json\n",
    "    with open(\"./c9_docs/\"+name, 'w', encoding=\"utf8\") as outfile:\n",
    "        json.dump(obj, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lemmas_raw():\n",
    "    with open(\"./c9_docs/lemmas.txt\", 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    lemmas = {}\n",
    "    for line in lines:\n",
    "        if line != \"\":\n",
    "            words = [word.strip() for word in line.split()]\n",
    "            wordform = words[0].replace(\"#\", \"\")\n",
    "            lemmas[wordform] = words[-1]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromJson(name):\n",
    "    import json\n",
    "    with open(\"./c9_docs/\"+name, 'r', encoding=\"utf8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emod', 'htm', 'http', 'www', 'excelsior', 'mx', 'art', 'html', 'excelsior', 'editorial', 'jueves', 'octubre', 'epigrama', 'jorge', 'mansilla', 'torres', 'critica', 'miami', 'herald', 'presidente', 'ecuatoriano', 'autoproclamarse', 'loco', 'neoliberalismo', 'aplica', 'encomio', 'hace', 'mismo', 'país', 'manicomio', 'editorial', 'nota', 'siguiente', 'http', 'www', 'excelsior', 'mx', 'art', 'html', 'excelsior', 'editorial', 'jueves', 'octubre', 'hungría', 'rebelión', 'antiestalinista', 'oscar', 'gonzalez', 'lopez', 'curso', 'octubre', 'insurgencia', 'popular', 'comandada', 'estudiantes', 'intelectuales', 'obreros', 'partidarios', 'establecer', 'suelo', 'húngaro', 'régimen', 'socialista', 'libertad', 'creció', 'vertiginosamente', 'desembocar', 'auténtica', 'insurrección', 'armada', 'estructura', 'estalinista', 'surgido', 'consecuencia', 'resultados', 'primera', 'guerra', 'mundial', 'años', 'distancia', 'conviene', 'analizar', 'acontecimientos', 'otoño', 'magyar', 'dada', 'conducta', 'adoptada', 'gobierno', 'urss', 'partidos', 'comunistas', 'poder', 'actividad', 'parlamentaria', 'influencia', 'innegable', 'clase', 'trabajadora', 'países', 'europa', 'occidental', 'francia', 'italia', 'lucha', 'librada', 'condiciones', 'clandestinidad', 'persecución', 'dictaduras', 'españa', 'grecia', 'portugal', 'asentadas', 'patrocinio', 'estadunidense', 'mayoría', 'naciones', 'latinoamericanas', 'asombra', 'verdad', 'años', 'distancia', 'todavía', 'dado', 'yugulamiento', 'experiencia', 'checoslovaca', 'construir', 'socialismo', 'rostro', 'humano', 'primavera', 'mayoría', 'izquierda', 'comunista', 'aceptara', 'todas', 'latitudes', 'punto', 'vista', 'ideólogos', 'políticos', 'partido', 'comunista', 'urss', 'repitiera', 'hechos', 'ámbito', 'acción', 'militante', 'izquierda', 'años', 'escribe', 'líneas', 'asombra', 'verdad', 'así', 'segunda', 'mitad', 'años', 'intelectuales', 'renombre', 'mundial', 'simpatizantes', 'socialismo', 'soviético', 'partidos', 'comunistas', 'aceptaran', 'mayor', 'análisis', 'puntos', 'vista', 'moscú', 'execrables', 'procesos', 'terminaron', 'liquidación', 'lúcidos', 'probados', 'cuadros', 'políticos', 'militares', 'pcus', 'causaron', 'necrosis', 'socialismo', 'llamado', 'real', 'asombra', 'admitiera', 'septiembre', 'sarta', 'calumnias', 'gran', 'revolucionario', 'héroe', 'clase', 'trabajadora']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./c8_docs/e961024.htm\", \"r\", encoding=\"utf8\")\n",
    "org_txt = f.read()\n",
    "f.close()\n",
    "\n",
    "text = delete_trash(split_text(clean_html(org_txt)))\n",
    "normalized_tokens = delete_stopwords(nltk.word_tokenize(text))\n",
    "print(normalized_tokens[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diccionario de lemas\n",
    "Tomaremos el archivo **lemmas.txt** que contiene un mapa de palabras comunes en una sintáxis que nos permitirá obtener los lemas. \n",
    "Imprimiremos algunos valores del diccionario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] = !\n",
      "[\"] = \"\n",
      "[%] = %\n",
      "['] = '\n",
      "[(] = (\n",
      "[)] = )\n",
      "[,] = ,\n",
      "[-] = -\n",
      "[.] = .\n",
      "[...] = ...\n",
      "[/] = /\n",
      "[:] = :\n",
      "[;] = ;\n",
      "[?] = ?\n",
      "[[] = [\n",
      "[]] = ]\n",
      "[a] = a\n",
      "[a-já] = a-já\n",
      "[abad] = abad\n",
      "[abades] = abad\n",
      "[abadesa] = abadesa\n",
      "[abadesas] = abadesa\n",
      "[abajo] = abajo\n",
      "[abalance] = abalanzar\n",
      "[abalancemos] = abalanzar\n",
      "[abalancen] = abalanzar\n",
      "[abalances] = abalanzar\n",
      "[abalancé] = abalanzar\n",
      "[abalancéis] = abalanzar\n",
      "[abalancémonos] = abalanzar\n",
      "[abalancémonosla] = abalanzar\n",
      "[abalancémonoslas] = abalanzar\n",
      "[abalancémonosle] = abalanzar\n",
      "[abalancémonosles] = abalanzar\n",
      "[abalancémonoslo] = abalanzar\n",
      "[abalancémonoslos] = abalanzar\n",
      "[abalancémosla] = abalanzar\n",
      "[abalancémoslas] = abalanzar\n",
      "[abalancémosle] = abalanzar\n",
      "[abalancémosles] = abalanzar\n",
      "[abalancémoslo] = abalanzar\n",
      "[abalancémoslos] = abalanzar\n",
      "[abalancémosmela] = abalanzar\n",
      "[abalancémosmelas] = abalanzar\n",
      "[abalancémosmele] = abalanzar\n",
      "[abalancémosmeles] = abalanzar\n",
      "[abalancémosmelo] = abalanzar\n",
      "[abalancémosmelos] = abalanzar\n",
      "[abalancémossela] = abalanzar\n",
      "[abalancémosselas] = abalanzar\n"
     ]
    }
   ],
   "source": [
    "lemmas_dic = load_lemmas_raw()\n",
    "for item in sorted(lemmas_dic.items())[:50]:\n",
    "    print(F\"[{item[0]}] = {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpToJson(lemmas_dic, \"lemmas_dic.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, este diccionario de lemas, nos permite obtener el lema de un conjunto de palabras, lo cual nos permitirá sustituir las palabras de nuestros tokens normalizados a tokens lematizados. \n",
    "\n",
    "### Reemplazo de Tokens por Lemas\n",
    "Ahora, crearemos una función que nos permita sustituir los tokens que tenemos por unos lematizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_lemmas(tokens):\n",
    "    lemmas = loadFromJson(\"lemmas_dic.json\")\n",
    "    lemmatized_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_tokens.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_tokens.append(word)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emod', 'htm', 'http', 'www', 'excelsior', 'mx', 'art', 'html', 'excelsior', 'editorial', 'jueves', 'octubre', 'epigrama', 'jorge', 'mansilla', 'torre', 'criticar', 'miami', 'herald', 'presidente', 'ecuatoriano', 'autoproclamarse', 'loco', 'neoliberalismo', 'aplicar', 'encomio', 'hacer', 'mismo', 'país', 'manicomio', 'editorial', 'nota', 'siguiente', 'http', 'www', 'excelsior', 'mx', 'art', 'html', 'excelsior', 'editorial', 'jueves', 'octubre', 'hungría', 'rebelión', 'antiestalinista', 'oscar', 'gonzalez', 'lopez', 'curso', 'octubre', 'insurgencia', 'popular', 'comandada', 'estudiante', 'intelectual', 'obrero', 'partidario', 'establecer', 'soler', 'húngaro', 'régimen', 'socialista', 'libertad', 'crecer', 'vertiginosamente', 'desembocar', 'auténtica', 'insurrección', 'armada', 'estructura', 'estalinista', 'surgir', 'consecuencia', 'resultado', 'primero', 'guerra', 'mundial', 'año', 'distancia', 'convenir', 'analizar', 'acontecimiento', 'otoño', 'magyar', 'dado', 'conducta', 'adoptar', 'gobierno', 'urss', 'partido', 'comunista', 'poder', 'actividad', 'parlamentaria', 'influencia', 'innegable', 'clase', 'trabajadora', 'país']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_tokens = replace_with_lemmas(normalized_tokens)\n",
    "print(lemmatized_tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, ahora tenemos nuestros tokens lematizados y listos para obtener un resultado mas certero en la búsqueda de similitudes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
