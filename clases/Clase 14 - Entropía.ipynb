{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 14\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "    \n",
    "## Entropía\n",
    "La entropía, es una medida que nos dice la incertirumbre que tiene un mensaje. Es decir, podemos saber la que tan incierto es que recibamos un determinado mensaje dado un contexto. En el caso del lenguaje natural, lo utilizamos para saber que tan incierto es que una palabra aparezca en en una posición dentro de una oración, por lo que necesitaremos tokens etiquetados con POS para poder procesar la entropía.  \n",
    "\n",
    "Podemos ver como una oración como:\n",
    "Mi $X$ come $Y$ muy contento.\n",
    "puede tomar diferentes valores como $X = ($ *gato, perro, raton, erizo* $)$, o $Y = ($ *croquetas, carne, zanahorias* $)$\n",
    "sin embargo, también podría darse valores como $X=$ *chimpancé*, pero no será un caso muy probable. \n",
    "Justo es aquí donde la entropía nos ayuda, ya que nos permite saber que tan incierto es que aparezca chimpancé como una mascota.\n",
    "Varios factores pueden afectar a esto, y uno principal son las *relaciones sintagmáticas* que nos van a ayudará en el aumento de la incertidumbre de que aparezca por ejemplo, un pronombre en donde debería hacer un sujeto.\n",
    "\n",
    "### Modelos predictivos\n",
    "Al conocer el nivel de entropía de una palabra dado un contexto anterior podemos crear un conjunto de palabras con baja incertidumbre que podrían estar a continuación de nuestra oración tal como lo hacen los teclados predictores de texto. \n",
    "\n",
    "### Incertidumbre\n",
    "Para averiguar la incertidumbre de que dada una palabra aparezca otra, deberemos manejar 2 estados, en los que $0$ indicará que **no** aparece en nuestro documento y $1$ indicar que **si** aparece en nuestro documento.\n",
    "\n",
    "$$ P(X=0) + P(X=1) = 1 $$\n",
    "\n",
    "Mientras más aleatoria sea $X$ más difil será de predecir su aparición. \n",
    "\n",
    "Para calcular la entropía o *incertidumbre* deberemos aplicar esta formula:\n",
    "$$\n",
    "H(X_{w}) = \\sum_{v=\\{0,1\\}}{-p(X_{w} = v) * \\log_{2}{p(X_{w} = 1)}}\n",
    "$$\n",
    "\n",
    "**Práctica** -> Calcular las palabras que pueden estar relacionadas a una *palabra objetivo* utilizando entropía."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
