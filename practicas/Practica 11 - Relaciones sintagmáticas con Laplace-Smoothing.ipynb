{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 11\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "## Relaciones sintagmáticas con Laplace-Smoothing\n",
    "**Instrucciones:**\n",
    "    Utilizar el práctica 10, pero implementando el cálculo de probabilidades con Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import math\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(txt):\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\treturn BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteTrash(txt):\n",
    "\timport re\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in txt:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitToSentences(txt):\n",
    "    tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "    return tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopwords(txt):\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tans = []\n",
    "\tstp = stopwords.words()\n",
    "\tfor w in txt:\n",
    "\t\tif w not in stp:\n",
    "\t\t\tans.append(w)\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary(sentences):\n",
    "    tokens = set()\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens.add(token)\n",
    "    return sorted(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencies(sentences, vocabulary):\n",
    "    tokens = dict()\n",
    "    \n",
    "    for token in vocabulary:\n",
    "        tokens[token] = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens[token] += 1\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContexts(wordList, windowSize=4):\n",
    "    contexts = dict()\n",
    "    index = 0\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if word not in contexts:\n",
    "            contexts[word] = []\n",
    "        start = max(0, index - windowSize)\n",
    "        end = min(index + windowSize, len(wordList) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i != index:\n",
    "                contexts[word].append(wordList[i])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromJson(name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'r', encoding=\"utf8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpToJson(obj, name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'w', encoding=\"utf8\") as outfile:\n",
    "        json.dump(obj, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWithStems(tokens):\n",
    "    ss = SnowballStemmer(\"spanish\")\n",
    "    stematized_tokens = []\n",
    "    for word in tokens:\n",
    "        stematized_tokens.append(ss.stem(word))\n",
    "    return stematized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacWithLemmas(tokens):\n",
    "    lemmas = loadFromJson(\"lemmatized_tokens_generate\")\n",
    "    lemmatized_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_tokens.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_tokens.append(word)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTagger():\n",
    "    patterns = [\n",
    "        (r'.*o$', 'n'),  # noun masculine singular\n",
    "        (r'.*os$', 'n'), # noun masculine plural\n",
    "        (r'.*a$', 'n'),  # noun femenine singular\n",
    "        (r'.*as$', 'n')  # noun femenine plural\n",
    "    ]\n",
    "    regexTagger = nltk.RegexpTagger(patterns, nltk.DefaultTagger('s'))\n",
    "    unigramTagger = nltk.UnigramTagger(nltk.corpus.cess_esp.tagged_sents(), None, regexTagger)\n",
    "    return unigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixTags(tokens, tokenTags):\n",
    "    taggedTokens = list()\n",
    "    for i in range(0,len(tokens)):\n",
    "        taggedTokens.append((tokens[i], tokenTags[i][1]))\n",
    "    return taggedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeSencences(sent):\n",
    "    tagger = getTagger()\n",
    "    sentences = []\n",
    "    for s in sent:\n",
    "        cleanSentence = deleteTrash(splitText(s))\n",
    "        normalizedTokens = deleteStopwords(nltk.word_tokenize(cleanSentence))\n",
    "        tokenTags = tagger.tag(normalizedTokens)\n",
    "        stematizedTokens = replaceWithStems(normalizedTokens)\n",
    "        tokens = mixTags(stematizedTokens, tokenTags)\n",
    "        sentences.append(tokens)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./files/e961024.htm\", \"r\", encoding=\"utf8\")\n",
    "org_txt = f.read()\n",
    "f.close()\n",
    "\n",
    "sentences = splitToSentences(cleanHtml(org_txt))\n",
    "sentences = normalizeSencences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = getVocabulary(sentences)\n",
    "frequencies = getFrequencies(sentences, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando la Entropía\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntropy(tgP, wordP, target, word, n, sentences):\n",
    "    prob_11 = 0\n",
    "    for sent in sentences:\n",
    "        if (target in sent) and (word in sent):\n",
    "             prob_11 += 1\n",
    "    prob_11 /= n\n",
    "    \n",
    "    tgQ = 1 - tgP\n",
    "    wordQ = 1 - wordP\n",
    "    \n",
    "    prob_01 = wordP - prob_11\n",
    "    prob_10 = tgP - prob_11\n",
    "    prob_00 = tgQ - prob_01\n",
    "    \n",
    "    #print(\"prob => \",prob_00, prob_01, prob_10, prob_11)\n",
    "    \n",
    "    a,b,c,d = 0,0,0,0\n",
    "    if wordQ != 0:\n",
    "        if prob_00/wordQ > 0:\n",
    "            a = prob_00 * math.log2(prob_00 / wordQ)\n",
    "        if prob_10/wordQ > 0:\n",
    "            b = prob_10 * math.log2(prob_10 / wordQ)\n",
    "    if wordP != 0:\n",
    "        if prob_01/wordP > 0:\n",
    "            c = prob_01 * math.log2(prob_01/wordP)\n",
    "        if prob_11/wordP > 0:\n",
    "            d = prob_11 * math.log2(prob_11/wordP)\n",
    "            \n",
    "    #print(\"entropy => \",a,b,c,d)\n",
    "            \n",
    "    return (a+b+c+d)*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarsTo(target, frequency, vocabulary, sentences):\n",
    "    w = SnowballStemmer(\"spanish\").stem(target)\n",
    "    n = len(sentences)\n",
    "    k = 2 #Laplace Smoothing variable\n",
    "    \n",
    "    targetTokens = set()\n",
    "    for token in vocabulary:\n",
    "        if token[0] == w:\n",
    "            targetTokens.add(token)\n",
    "    \n",
    "    print(\"Checking for \" + str(len(targetTokens)) + \" tokens of word \" + target)\n",
    "    for tg in targetTokens:\n",
    "        print(\"\\t\" + str(tg))\n",
    "            \n",
    "    similarity = dict()\n",
    "    for word in vocabulary:\n",
    "        similarity[word] = 0.0\n",
    "    for tg in targetTokens:\n",
    "        tgProb = (frequency[tg]+k)/(n+frequency[tg]*k)\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            wProb = (frequency[word]+k)/(n+frequency[tg]*k)\n",
    "            entropy = getEntropy(tgProb, wProb, tg, word, n, sentences)\n",
    "            similarity[word] += entropy\n",
    "    \n",
    "    relatedWords = []\n",
    "    for token,entropy in similarity.items():\n",
    "        relatedWords.append((entropy,token))\n",
    "    \n",
    "    return sorted(relatedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 8 tokens of word economía\n",
      "\t('econom', 'nccp000')\n",
      "\t('econom', 'ncfp000')\n",
      "\t('econom', 'nccs000')\n",
      "\t('econom', 'aq0fp0')\n",
      "\t('econom', 'aq0fs0')\n",
      "\t('econom', 'aq0mp0')\n",
      "\t('econom', 'ncfs000')\n",
      "\t('econom', 'aq0ms0')\n"
     ]
    }
   ],
   "source": [
    "result = getSimilarsTo(\"economía\", frequencies, vocabulary, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42399246024553844, ('econom', 'ncfs000')\n",
      "0.42822974710363493, ('econom', 'aq0fs0')\n",
      "0.45796703422891827, ('econom', 'aq0ms0')\n",
      "0.5128642448178622, ('econom', 'aq0mp0')\n",
      "0.5146568179775968, ('econom', 'aq0fp0')\n",
      "0.5202227051868888, ('econom', 'nccs000')\n",
      "0.5302103621507589, ('econom', 'nccp000')\n",
      "0.5311006299698519, ('ven', 'vmip3p0')\n",
      "0.5324321609685821, ('recuper', 'ncfs000')\n",
      "0.5328300127486351, ('crecimient', 'ncms000')\n",
      "0.5365953606807705, ('econom', 'ncfp000')\n",
      "0.5370202381498146, ('globaliz', 'ncfs000')\n",
      "0.537106565969665, ('polit', 'ncfs000')\n",
      "0.5372927380917684, ('pais', 'ncms000')\n",
      "0.5373240347338064, ('funcionari', 'ncmp000')\n",
      "0.5374473724273325, ('mundial', 'aq0cs0')\n",
      "0.5374557268143445, ('financier', 'aq0mp0')\n",
      "0.5376705268498186, ('perversion', 's')\n",
      "0.53786814734084, ('conoc', 'ncms000')\n",
      "0.5380907366992723, ('crisis', 'ncfn000')\n",
      "0.5381400034191206, ('bm', 's')\n",
      "0.538146611721232, ('mexican', 'aq0fs0')\n",
      "0.5382471485198589, ('polit', 'aq0fp0')\n",
      "0.5385303420222378, ('banc', 'ncms000')\n",
      "0.5386003911020913, ('error', 'ncms000')\n",
      "0.5386317670339852, ('internacional', 'aq0cs0')\n",
      "0.5386482003376735, ('product', 'ncmp000')\n",
      "0.5387687474176319, ('centr', 'ncms000')\n",
      "0.538809777883196, ('not', 'ncfs000')\n",
      "0.5388250239304542, ('educ', 'ncfs000')\n",
      "0.5389165640994328, ('vez', 'ncfs000')\n",
      "0.5389172864366796, ('requier', 'vmip3s0')\n",
      "0.5390102915630545, ('siguient', 'aq0cs0')\n",
      "0.5390269505350229, ('sin', 'cc')\n",
      "0.5391075092107372, ('html', 's')\n",
      "0.5391075092107372, ('http', 's')\n",
      "0.5391075092107372, ('mx', 's')\n",
      "0.5391075092107372, ('www', 's')\n",
      "0.5392305750110172, ('public', 'aq0ms0')\n",
      "0.539323787424706, ('bas', 'aq0fsp')\n",
      "0.5393371472967755, ('autor', 'ncms000')\n",
      "0.5393621614531079, ('porcentaj', 'ncms000')\n",
      "0.5394065725127076, ('model', 'ncms000')\n",
      "0.5394346569024969, ('confianz', 'ncfs000')\n",
      "0.5394439796061467, ('estabil', 'ncfs000')\n",
      "0.5394462662318059, ('cas', 'ncms000')\n",
      "0.5394472656668496, ('nuev', 'aq0fs0')\n",
      "0.5394678254309622, ('pes', 'ncms000')\n",
      "0.5394773114559138, ('juev', 'W')\n",
      "0.5394854650415801, ('import', 'aq0cs0')\n",
      "0.5395288901024425, ('asi', 'rg')\n",
      "0.5396436766144624, ('desemple', 'ncms000')\n",
      "0.5397560835138692, ('cad', 'di0cs0')\n",
      "0.5397595234977015, ('macroeconom', 'n')\n",
      "0.5398034924939291, ('ct', 's')\n",
      "0.5398657231374812, ('public', 'aq0mp0')\n",
      "0.5399914298751503, ('perspect', 'ncfs000')\n",
      "0.5400067182434798, ('ingres', 'ncmp000')\n",
      "0.540026296301489, ('mal', 'aq0mp0')\n",
      "0.540026296301489, ('mit', 'ncmp000')\n",
      "0.540026296301489, ('paz', 'n')\n",
      "0.540026296301489, ('valor', 'vmip3p0')\n",
      "0.540114097938979, ('eugeni', 'n')\n",
      "0.540114097938979, ('kuztnetsov', 's')\n",
      "0.540217619238413, ('institut', 'ncms000')\n",
      "0.5402222340353345, ('telecomun', 'ncfp000')\n",
      "0.5402390105662894, ('octubr', 'W')\n",
      "0.5402420239955457, ('proyect', 'ncms000')\n",
      "0.5402623706895778, ('salari', 'ncmp000')\n",
      "0.5402763959584627, ('diferent', 'ncfs000')\n",
      "0.5403113852134962, ('actual', 'aq0cs0')\n",
      "0.5403413484161197, ('efect', 'ncmp000')\n",
      "0.5404466837592156, ('residual', 'aq0cp0')\n",
      "0.5404490206107788, ('estrategi', 'ncfs000')\n",
      "0.5405080898356864, ('visibl', 'aq0cp0')\n",
      "0.54054452765206, ('añad', 'vmis3s0')\n",
      "0.5405533678596564, ('financ', 'aq0fs0')\n",
      "0.5405943725837057, ('bursatil', 'aq0cs0')\n",
      "0.5406874742293785, ('segund', 'ao0fs0')\n",
      "0.540732727862065, ('separ', 'vmp00sm')\n",
      "0.5407371005687704, ('gran', 'aq0cs0')\n",
      "0.5407765963836559, ('situacion', 'ncfs000')\n",
      "0.5407895300844494, ('lueg', 'rg')\n",
      "0.5407955119301423, ('condicion', 'ncfp000')\n",
      "0.5407957376371328, ('comerci', 'ncms000')\n",
      "0.5408053714087253, ('logr', 's')\n",
      "0.540819316453921, ('impuest', 'ncmp000')\n",
      "0.5408257758965791, ('zedill', 'n')\n",
      "0.5408761225401315, ('agu', 'ncfp000')\n",
      "0.5408761225401315, ('refir', 'vmis3s0')\n",
      "0.5408923053075816, ('carreon', 's')\n",
      "0.5408923053075816, ('depend', 'vmip3p0')\n",
      "0.5408923053075816, ('desarroll', 'aq0fpp')\n",
      "0.5409060312144881, ('posibl', 'aq0cp0')\n",
      "0.5409137714001274, ('gravisim', 'n')\n",
      "0.5409673902132713, ('acontec', 's')\n",
      "0.540981991908394, ('mayor', 'aq0cp0')\n",
      "0.5409883013593566, ('entorn', 'ncms000')\n",
      "0.5410517559533553, ('ver', 'vmn0000')\n",
      "0.541061349311374, ('sistem', 'ncms000')\n"
     ]
    }
   ],
   "source": [
    "for r in result[:100]:\n",
    "    print(str(r[0]) + \", \" + str(r[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar *Laplace-Smoothing*, podemos notar que algunas palabras como las variantes de economía ahorase encuentran complementamente arriba, mientras que con la entropía de probabilidad estandar algunas aparecían en diferentes posiciones a pesar de tratarse de la misma palabra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 1 tokens of word acero\n",
      "\t('acer', 'ncms000')\n"
     ]
    }
   ],
   "source": [
    "result2 = getSimilarsTo(\"acero\", frequencies, vocabulary, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013764969652885355, ('acer', 'ncms000')\n",
      "0.013764969652885355, ('barat', 'aq0fs0')\n",
      "0.013764969652885355, ('cabl', 's')\n",
      "0.013764969652885355, ('cobr', 'ncms000')\n",
      "0.013764969652885355, ('codif', 's')\n",
      "0.013764969652885355, ('desmaterializ', 'n')\n",
      "0.013764969652885355, ('distanci', 'ncfp000')\n",
      "0.013764969652885355, ('fibr', 'n')\n",
      "0.013764969652885355, ('impuls', 'vmp00sm')\n",
      "0.013764969652885355, ('larg', 'aq0fp0')\n",
      "0.013764969652885355, ('microproces', 's')\n",
      "0.013764969652885355, ('mov', 'vmn0000')\n",
      "0.013764969652885355, ('noved', 's')\n",
      "0.013764969652885355, ('optic', 'n')\n",
      "0.013764969652885355, ('pes', 'vmp00sm')\n",
      "0.013764969652885355, ('rasg', 'n')\n",
      "0.013764969652885355, ('transistor', 's')\n",
      "0.013764969652885355, ('traslad', 'vmp00sm')\n",
      "0.013764969652885355, ('tub', 'ncmp000')\n",
      "0.014031032738063744, ('aceler', 'n')\n",
      "0.014031032738063744, ('cabl', 'ncms000')\n",
      "0.014031032738063744, ('digital', 'aq0cs0')\n",
      "0.014031032738063744, ('vaci', 'ncms000')\n",
      "0.01422864397416685, ('transmision', 'ncfs000')\n",
      "0.014516603539128066, ('tecnolog', 'aq0ms0')\n",
      "0.014628334270329512, ('bas', 'aq0fsp')\n",
      "0.014628334270329512, ('intens', 'aq0fs0')\n",
      "0.014725910634361464, ('diferent', 'ncfs000')\n",
      "0.014725910634361464, ('facil', 'aq0cs0')\n",
      "0.014812500513348845, ('industrial', 'aq0cs0')\n",
      "0.014960953253943476, ('epoc', 'ncfs000')\n",
      "0.01508523407218166, ('esta', 'pd0fs000')\n",
      "0.015192058136555973, ('conoc', 'ncms000')\n",
      "0.015240315558567459, ('cos', 'ncfp000')\n",
      "0.01532847238559493, ('actual', 'aq0cs0')\n",
      "0.015512077320545433, ('siempr', 'rg')\n",
      "0.015574503966983792, ('econom', 'aq0ms0')\n",
      "0.015574503966983792, ('respect', 'ncms000')\n",
      "0.015659268856846632, ('sid', 'vsp00sm')\n",
      "0.015710805027022573, ('bien', 'rg')\n",
      "0.015735275810841463, ('gran', 'aq0cs0')\n",
      "0.015781887622226532, ('produccion', 'ncfs000')\n",
      "0.01580412046916332, ('econom', 'ncfs000')\n",
      "0.015886796820039815, ('hac', 'vmn0000')\n",
      "0.015978332267353734, ('ahor', 'rg')\n",
      "0.01604392218138795, ('asi', 'rg')\n",
      "0.01604392218138795, ('cambi', 'ncms000')\n",
      "0.01644653912016926, ('sol', 'rg')\n",
      "0.01720317357894599, ('mexic', 'np0000l')\n",
      "0.0172385758693779, ('millon', 'ncmp000')\n",
      "0.017242788579117374, ('excelsior', 's')\n",
      "0.017306456674395232, ('octubr', 'W')\n",
      "0.017315996061456722, ('merc', 'ncms000')\n",
      "0.01732414201653495, ('pais', 'ncms000')\n",
      "0.0173254969387577, ('pes', 'Zm')\n",
      "0.01733090884073288, ('año', 'ncms000')\n",
      "0.01733765626459595, ('cient', 'n')\n",
      "0.01733900343110643, ('gobiern', 'ncms000')\n",
      "0.017340349826926223, ('part', 'ncfs000')\n",
      "0.0173510933540598, ('juev', 'W')\n",
      "0.017353771590277674, ('dolar', 'Zm')\n",
      "0.017356446782178386, ('html', 's')\n",
      "0.017356446782178386, ('http', 's')\n",
      "0.017356446782178386, ('mx', 's')\n",
      "0.017356446782178386, ('www', 's')\n",
      "0.017357783238670894, ('siguient', 'aq0cs0')\n",
      "0.0173604538770503, ('not', 'ncfs000')\n",
      "0.0173631214883509, ('ser', 'vsn0000')\n",
      "0.017364454160989546, ('nacional', 'aq0cs0')\n",
      "0.01737110622820803, ('financ', 'aq0fs0')\n",
      "0.017376414377765357, ('president', 'ncms000')\n",
      "0.01737773954661266, ('años', 'ncmp000')\n",
      "0.017380387647943696, ('mil', 'dn0cp0')\n",
      "0.017383032773042026, ('agu', 'ncfs000')\n",
      "0.017388314121247795, ('fin', 'ncms000')\n",
      "0.01738963260856794, ('petroquim', 'n')\n",
      "0.01739095035765307, ('empres', 'ncfp000')\n",
      "0.01739095035765307, ('sin', 'cc')\n",
      "0.017392267369329098, ('iglesi', 'ncfs000')\n",
      "0.017392267369329098, ('primer', 'ao0fs0')\n",
      "0.017392267369329098, ('sector', 'ncms000')\n",
      "0.017397528058420306, ('banc', 'ncms000')\n",
      "0.017397528058420306, ('hoy', 'rg')\n",
      "0.017398841395396755, ('embarg', 'ncms000')\n",
      "0.017398841395396755, ('polit', 'ncfs000')\n",
      "0.017398841395396755, ('vez', 'ncfs000')\n",
      "0.017401465872724488, ('accion', 'ncfp000')\n",
      "0.017401465872724488, ('acuerd', 'ncms000')\n",
      "0.017401465872724488, ('dec', 'vmn0000')\n",
      "0.017401465872724488, ('dij', 'vmis3s0')\n",
      "0.017401465872724488, ('mexican', 'aq0ms0')\n",
      "0.017401465872724488, ('prim', 'ao0ms0')\n",
      "0.017402777014707663, ('desarroll', 'ncms000')\n",
      "0.017402777014707663, ('grup', 'ncms000')\n",
      "0.017402777014707663, ('mayor', 'aq0cs0')\n",
      "0.017404087426655952, ('banc', 'ncfs000')\n",
      "0.017404087426655952, ('nuev', 'aq0fs0')\n",
      "0.017404087426655952, ('sistem', 'ncms000')\n",
      "0.01740539710938179, ('crecimient', 'ncms000')\n",
      "0.01740539710938179, ('pes', 'ncms000')\n"
     ]
    }
   ],
   "source": [
    "for r in result2[:100]:\n",
    "    print(str(r[0]) + \", \" + str(r[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la prueba anterior, vimos que las primeras palabras tenían un nivel de entropía igual a 0, lo que es realmente imposible porque significa que tenemos total seguridad de que esa palabra siempre se encontrará junto a otra, sin embargo eso no falso, por lo que con *Laplace-Smoothing* podemos lograr obtener datos mas realisticos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
