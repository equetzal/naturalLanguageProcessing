{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 10\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "## Relaciones sintagmáticas con Entropy\n",
    "**Instrucciones:**\n",
    "    Ahora se leerá el texto como párrafos a los que llamaremos *sentence*, y para cada uno deberemos realizar nuestro proceso de normalización\n",
    "    Utilizar derivación y el modelo BM25, pero esta vez con etiquedado POS y obtener la similitud con los vectores del model BM25 pero utilizando la relación de tagging para una determinada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import math\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(txt):\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\treturn BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteTrash(txt):\n",
    "\timport re\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in txt:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitToSentences(txt):\n",
    "    tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "    return tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopwords(txt):\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tans = []\n",
    "\tstp = stopwords.words()\n",
    "\tfor w in txt:\n",
    "\t\tif w not in stp:\n",
    "\t\t\tans.append(w)\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary(sentences):\n",
    "    tokens = set()\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens.add(token)\n",
    "    return sorted(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencies(sentences, vocabulary):\n",
    "    tokens = dict()\n",
    "    \n",
    "    for token in vocabulary:\n",
    "        tokens[token] = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens[token] += 1\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContexts(wordList, windowSize=4):\n",
    "    contexts = dict()\n",
    "    index = 0\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if word not in contexts:\n",
    "            contexts[word] = []\n",
    "        start = max(0, index - windowSize)\n",
    "        end = min(index + windowSize, len(wordList) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i != index:\n",
    "                contexts[word].append(wordList[i])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromJson(name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'r', encoding=\"utf8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpToJson(obj, name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'w', encoding=\"utf8\") as outfile:\n",
    "        json.dump(obj, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWithStems(tokens):\n",
    "    ss = SnowballStemmer(\"spanish\")\n",
    "    stematized_tokens = []\n",
    "    for word in tokens:\n",
    "        stematized_tokens.append(ss.stem(word))\n",
    "    return stematized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacWithLemmas(tokens):\n",
    "    lemmas = loadFromJson(\"lemmatized_tokens_generate\")\n",
    "    lemmatized_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_tokens.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_tokens.append(word)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTagger():\n",
    "    patterns = [\n",
    "        (r'.*o$', 'n'),  # noun masculine singular\n",
    "        (r'.*os$', 'n'), # noun masculine plural\n",
    "        (r'.*a$', 'n'),  # noun femenine singular\n",
    "        (r'.*as$', 'n')  # noun femenine plural\n",
    "    ]\n",
    "    regexTagger = nltk.RegexpTagger(patterns, nltk.DefaultTagger('s'))\n",
    "    unigramTagger = nltk.UnigramTagger(nltk.corpus.cess_esp.tagged_sents(), None, regexTagger)\n",
    "    return unigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixTags(tokens, tokenTags):\n",
    "    taggedTokens = list()\n",
    "    for i in range(0,len(tokens)):\n",
    "        taggedTokens.append((tokens[i], tokenTags[i][1]))\n",
    "    return taggedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeSencences(sent):\n",
    "    tagger = getTagger()\n",
    "    sentences = []\n",
    "    for s in sent:\n",
    "        cleanSentence = deleteTrash(splitText(s))\n",
    "        normalizedTokens = deleteStopwords(nltk.word_tokenize(cleanSentence))\n",
    "        tokenTags = tagger.tag(normalizedTokens)\n",
    "        stematizedTokens = replaceWithStems(normalizedTokens)\n",
    "        tokens = mixTags(stematizedTokens, tokenTags)\n",
    "        sentences.append(tokens)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./files/e961024.htm\", \"r\", encoding=\"utf8\")\n",
    "org_txt = f.read()\n",
    "f.close()\n",
    "\n",
    "sentences = splitToSentences(cleanHtml(org_txt))\n",
    "sentences = normalizeSencences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = getVocabulary(sentences)\n",
    "frequencies = getFrequencies(sentences, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando la Entropía\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntropy(tgP, wordP, target, word, n, sentences):\n",
    "    prob_11 = 0\n",
    "    for sent in sentences:\n",
    "        if (target in sent) and (word in sent):\n",
    "             prob_11 += 1\n",
    "    prob_11 /= n\n",
    "    \n",
    "    tgQ = 1 - tgP\n",
    "    wordQ = 1 - wordP\n",
    "    \n",
    "    prob_01 = wordP - prob_11\n",
    "    prob_10 = tgP - prob_11\n",
    "    prob_00 = tgQ - prob_01\n",
    "    \n",
    "    #print(\"prob => \",prob_00, prob_01, prob_10, prob_11)\n",
    "    \n",
    "    a,b,c,d = 0,0,0,0\n",
    "    if wordQ != 0:\n",
    "        if prob_00/wordQ > 0:\n",
    "            a = prob_00 * math.log2(prob_00 / wordQ)\n",
    "        if prob_10/wordQ > 0:\n",
    "            b = prob_10 * math.log2(prob_10 / wordQ)\n",
    "    if wordP != 0:\n",
    "        if prob_01/wordP > 0:\n",
    "            c = prob_01 * math.log2(prob_01/wordP)\n",
    "        if prob_11/wordP > 0:\n",
    "            d = prob_11 * math.log2(prob_11/wordP)\n",
    "            \n",
    "    #print(\"entropy => \",a,b,c,d)\n",
    "            \n",
    "    return (a+b+c+d)*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarsTo(target, frequency, vocabulary, sentences):\n",
    "    w = SnowballStemmer(\"spanish\").stem(target)\n",
    "    n = len(sentences)\n",
    "    \n",
    "    targetTokens = set()\n",
    "    for token in vocabulary:\n",
    "        if token[0] == w:\n",
    "            targetTokens.add(token)\n",
    "    \n",
    "    print(\"Checking for \" + str(len(targetTokens)) + \" tokens of word \" + target)\n",
    "    for tg in targetTokens:\n",
    "        print(\"\\t\" + str(tg))\n",
    "            \n",
    "    similarity = dict()\n",
    "    for word in vocabulary:\n",
    "        similarity[word] = 0.0\n",
    "    for tg in targetTokens:\n",
    "        tgProb = frequency[tg]/n\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            wProb = frequency[word]/n\n",
    "            entropy = getEntropy(tgProb, wProb, tg, word, n, sentences)\n",
    "            similarity[word] += entropy\n",
    "    \n",
    "    relatedWords = []\n",
    "    for token,entropy in similarity.items():\n",
    "        relatedWords.append((entropy,token))\n",
    "    \n",
    "    return sorted(relatedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 8 tokens of word economía\n",
      "\t('econom', 'aq0ms0')\n",
      "\t('econom', 'aq0mp0')\n",
      "\t('econom', 'ncfp000')\n",
      "\t('econom', 'ncfs000')\n",
      "\t('econom', 'nccp000')\n",
      "\t('econom', 'aq0fs0')\n",
      "\t('econom', 'nccs000')\n",
      "\t('econom', 'aq0fp0')\n"
     ]
    }
   ],
   "source": [
    "result = getSimilarsTo(\"economía\", frequencies, vocabulary, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3641814132108909, ('econom', 'ncfs000')\n",
      "0.3675207047273351, ('econom', 'aq0fs0')\n",
      "0.39793888255221943, ('econom', 'aq0ms0')\n",
      "0.45103438653804856, ('econom', 'aq0mp0')\n",
      "0.4524651492150109, ('econom', 'aq0fp0')\n",
      "0.45884011876521114, ('econom', 'nccs000')\n",
      "0.4696252930143442, ('econom', 'nccp000')\n",
      "0.47187853733975615, ('ven', 'vmip3p0')\n",
      "0.47709654375915966, ('recuper', 'ncfs000')\n",
      "0.4773778056725599, ('econom', 'ncfp000')\n",
      "0.4780838194149634, ('crecimient', 'ncms000')\n",
      "0.4807410909660985, ('perversion', 's')\n",
      "0.4809802425618901, ('conoc', 'ncms000')\n",
      "0.4812349214112581, ('funcionari', 'ncmp000')\n",
      "0.4814087199213799, ('globaliz', 'ncfs000')\n",
      "0.48160438161419983, ('bm', 's')\n",
      "0.4817505012620616, ('product', 'ncmp000')\n",
      "0.48190187955134867, ('educ', 'ncfs000')\n",
      "0.48215963140215146, ('mal', 'aq0mp0')\n",
      "0.48215963140215146, ('mit', 'ncmp000')\n",
      "0.48215963140215146, ('paz', 'n')\n",
      "0.48215963140215146, ('valor', 'vmip3p0')\n",
      "0.4821903602259055, ('pais', 'ncms000')\n",
      "0.48225256299422237, ('mundial', 'aq0cs0')\n",
      "0.4823480520608249, ('financier', 'aq0mp0')\n",
      "0.4823618592648271, ('polit', 'ncfs000')\n",
      "0.4824456051355358, ('autor', 'ncms000')\n",
      "0.48295854315374476, ('not', 'ncfs000')\n",
      "0.48300339761436795, ('polit', 'aq0fp0')\n",
      "0.48302423256769633, ('carreon', 's')\n",
      "0.48302423256769633, ('depend', 'vmip3p0')\n",
      "0.48302423256769633, ('desarroll', 'aq0fpp')\n",
      "0.4830713779908936, ('eugeni', 'n')\n",
      "0.4830713779908936, ('kuztnetsov', 's')\n",
      "0.4831762615158274, ('siguient', 'aq0cs0')\n",
      "0.48317733736861845, ('estabil', 'ncfs000')\n",
      "0.4832779243740912, ('error', 'ncms000')\n",
      "0.48328191929403447, ('html', 's')\n",
      "0.48328191929403447, ('http', 's')\n",
      "0.48328191929403447, ('mx', 's')\n",
      "0.48328191929403447, ('www', 's')\n",
      "0.4833381236406664, ('requier', 'vmip3s0')\n",
      "0.4834044647048378, ('internacional', 'aq0cs0')\n",
      "0.483435595736297, ('alin', 'vmp00pm')\n",
      "0.483435595736297, ('pragmat', 'n')\n",
      "0.483435595736297, ('profund', 'rg')\n",
      "0.483435595736297, ('rud', 'n')\n",
      "0.4834406703923576, ('bas', 'aq0fsp')\n",
      "0.48347699335051353, ('crisis', 'ncfn000')\n",
      "0.4834899270006913, ('mexican', 'aq0fs0')\n",
      "0.4835301223186535, ('model', 'ncms000')\n",
      "0.48354724204898086, ('public', 'aq0ms0')\n",
      "0.4836157993393912, ('banc', 'ncms000')\n",
      "0.48362481214778213, ('porcentaj', 'ncms000')\n",
      "0.48368449880668357, ('juev', 'W')\n",
      "0.4836868715240898, ('vez', 'ncfs000')\n",
      "0.483729526871009, ('centr', 'ncms000')\n",
      "0.4837350948287331, ('asi', 'rg')\n",
      "0.4838136115712191, ('ct', 's')\n",
      "0.4840603151267121, ('gary', 's')\n",
      "0.4840603151267121, ('hufbau', 's')\n",
      "0.4840603151267121, ('marc', 'aq0msp')\n",
      "0.48406408569759773, ('cas', 'ncms000')\n",
      "0.4841418978096556, ('desemple', 'ncms000')\n",
      "0.4841554603400009, ('public', 'aq0mp0')\n",
      "0.484288847003894, ('visibl', 'aq0cp0')\n",
      "0.484297552064931, ('telecomun', 'ncfp000')\n",
      "0.4843455831566377, ('confianz', 'ncfs000')\n",
      "0.4843865672774611, ('octubr', 'W')\n",
      "0.484404869871104, ('cad', 'di0cs0')\n",
      "0.4844452344604636, ('sin', 'cc')\n",
      "0.48465107707279537, ('macroeconom', 'n')\n",
      "0.48473888649932756, ('pes', 'ncms000')\n",
      "0.48482537005300474, ('efect', 'ncmp000')\n",
      "0.48484752593604297, ('ingres', 'ncmp000')\n",
      "0.4848682239662802, ('poderi', 'ncms000')\n",
      "0.48487642156405353, ('perspect', 'ncfs000')\n",
      "0.4848990829020119, ('nuev', 'aq0fs0')\n",
      "0.48491356709669076, ('posibl', 'aq0cp0')\n",
      "0.4849341521353877, ('separ', 'vmp00sm')\n",
      "0.4849819058137532, ('diferent', 'ncfs000')\n",
      "0.4850619369789968, ('residual', 'aq0cp0')\n",
      "0.4851116875172899, ('estrategi', 'ncfs000')\n",
      "0.4851419809212191, ('gravisim', 'n')\n",
      "0.48515345412779975, ('sinergi', 'n')\n",
      "0.48518282411525626, ('entorn', 'ncms000')\n",
      "0.48518498199795335, ('institut', 'ncms000')\n",
      "0.4852096331871256, ('acer', 'ncms000')\n",
      "0.4852096331871256, ('afect', 'vmif3s0')\n",
      "0.4852096331871256, ('barat', 'aq0fs0')\n",
      "0.4852096331871256, ('cabl', 's')\n",
      "0.4852096331871256, ('cobr', 'ncms000')\n",
      "0.4852096331871256, ('codif', 's')\n",
      "0.4852096331871256, ('coincid', 'vmis3s0')\n",
      "0.4852096331871256, ('desmaterializ', 'n')\n",
      "0.4852096331871256, ('distanci', 'ncfp000')\n",
      "0.4852096331871256, ('fibr', 'n')\n",
      "0.4852096331871256, ('impuls', 'vmp00sm')\n",
      "0.4852096331871256, ('larg', 'aq0fp0')\n",
      "0.4852096331871256, ('micr', 'n')\n"
     ]
    }
   ],
   "source": [
    "for r in result[:100]:\n",
    "    print(str(r[0]) + \", \" + str(r[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los reusltados podemos observar como el modelo de *Entropía*, igualmente arroja resultados similares a una palabra sin requetir los cosenos de todos los vectores.\n",
    "Valores simares a economía son arrejados, como lo son crecimiento, valor, país, macroecnonomía, valor , finanzas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 1 tokens of word acero\n",
      "\t('acer', 'ncms000')\n"
     ]
    }
   ],
   "source": [
    "result2 = getSimilarsTo(\"acero\", frequencies, vocabulary, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0, ('acer', 'ncms000')\n",
      "0.0, ('barat', 'aq0fs0')\n",
      "0.0, ('cabl', 's')\n",
      "0.0, ('cobr', 'ncms000')\n",
      "0.0, ('codif', 's')\n",
      "0.0, ('desmaterializ', 'n')\n",
      "0.0, ('distanci', 'ncfp000')\n",
      "0.0, ('fibr', 'n')\n",
      "0.0, ('impuls', 'vmp00sm')\n",
      "0.0, ('larg', 'aq0fp0')\n",
      "0.0, ('microproces', 's')\n",
      "0.0, ('mov', 'vmn0000')\n",
      "0.0, ('noved', 's')\n",
      "0.0, ('optic', 'n')\n",
      "0.0, ('pes', 'vmp00sm')\n",
      "0.0, ('rasg', 'n')\n",
      "0.0, ('transistor', 's')\n",
      "0.0, ('traslad', 'vmp00sm')\n",
      "0.0, ('tub', 'ncmp000')\n",
      "0.0010887316276537834, ('aceler', 'n')\n",
      "0.0010887316276537834, ('cabl', 'ncms000')\n",
      "0.0010887316276537834, ('digital', 'aq0cs0')\n",
      "0.0010887316276537834, ('vaci', 'ncms000')\n",
      "0.0014996665771167495, ('transmision', 'ncfs000')\n",
      "0.0019649648744892827, ('tecnolog', 'aq0ms0')\n",
      "0.002123099907398, ('bas', 'aq0fsp')\n",
      "0.002123099907398, ('intens', 'aq0fs0')\n",
      "0.0022546050354253088, ('diferent', 'ncfs000')\n",
      "0.0022546050354253088, ('facil', 'aq0cs0')\n",
      "0.0023671832039176763, ('industrial', 'aq0cs0')\n",
      "0.0025530516798542343, ('epoc', 'ncfs000')\n",
      "0.002703212957889823, ('esta', 'pd0fs000')\n",
      "0.0028292066265499516, ('conoc', 'ncms000')\n",
      "0.0028853511297339796, ('cos', 'ncfp000')\n",
      "0.0029868635281740706, ('actual', 'aq0cs0')\n",
      "0.0031947902730901324, ('siempr', 'rg')\n",
      "0.0032646570647776, ('econom', 'aq0ms0')\n",
      "0.0032646570647776, ('respect', 'ncms000')\n",
      "0.0033590255235412367, ('sid', 'vsp00sm')\n",
      "0.003416173675060477, ('bien', 'rg')\n",
      "0.003443260212060952, ('gran', 'aq0cs0')\n",
      "0.003494781915082675, ('produccion', 'ncfs000')\n",
      "0.0035193282192863187, ('econom', 'ncfs000')\n",
      "0.0036104923660032124, ('hac', 'vmn0000')\n",
      "0.0037113225734167134, ('ahor', 'rg')\n",
      "0.003783593164827021, ('asi', 'rg')\n",
      "0.003783593164827021, ('cambi', 'ncms000')\n",
      "0.004231683528941547, ('sol', 'rg')\n",
      "0.006603909875909134, ('mexic', 'np0000l')\n",
      "0.006615716380337806, ('millon', 'ncmp000')\n",
      "0.006617121307158497, ('excelsior', 's')\n",
      "0.006638354618473873, ('octubr', 'W')\n",
      "0.006641536033225869, ('merc', 'ncms000')\n",
      "0.006644252740051815, ('pais', 'ncms000')\n",
      "0.006644704612263261, ('pes', 'Zm')\n",
      "0.006646509505632494, ('año', 'ncms000')\n",
      "0.006648759804826957, ('cient', 'n')\n",
      "0.006649209091956728, ('gobiern', 'ncms000')\n",
      "0.006649658122204164, ('part', 'ncfs000')\n",
      "0.0066532411515446135, ('juev', 'W')\n",
      "0.0066541343604908055, ('dolar', 'Zm')\n",
      "0.006655026554714704, ('html', 's')\n",
      "0.006655026554714704, ('http', 's')\n",
      "0.006655026554714704, ('mx', 's')\n",
      "0.006655026554714704, ('www', 's')\n",
      "0.006655472272025794, ('siguient', 'aq0cs0')\n",
      "0.006656362948481966, ('not', 'ncfs000')\n",
      "0.006657252615958679, ('ser', 'vsn0000')\n",
      "0.006657697072043537, ('nacional', 'aq0cs0')\n",
      "0.006659915587306685, ('financ', 'aq0fs0')\n",
      "0.006661685898297701, ('president', 'ncms000')\n",
      "0.006662127853222562, ('años', 'ncmp000')\n",
      "0.006663011017645922, ('mil', 'dn0cp0')\n",
      "0.006663893190028092, ('agu', 'ncfs000')\n",
      "0.006665654567566254, ('fin', 'ncms000')\n",
      "0.006666094295394431, ('petroquim', 'n')\n",
      "0.006666533777152168, ('empres', 'ncfp000')\n",
      "0.006666533777152168, ('sin', 'cc')\n",
      "0.006666973013114658, ('iglesi', 'ncfs000')\n",
      "0.006666973013114658, ('primer', 'ao0fs0')\n",
      "0.006666973013114658, ('sector', 'ncms000')\n",
      "0.006668727504501614, ('banc', 'ncms000')\n",
      "0.006668727504501614, ('hoy', 'rg')\n",
      "0.006669165515600806, ('embarg', 'ncms000')\n",
      "0.006669165515600806, ('polit', 'ncfs000')\n",
      "0.006669165515600806, ('vez', 'ncfs000')\n",
      "0.006670040805610944, ('accion', 'ncfp000')\n",
      "0.006670040805610944, ('acuerd', 'ncms000')\n",
      "0.006670040805610944, ('dec', 'vmn0000')\n",
      "0.006670040805610944, ('dij', 'vmis3s0')\n",
      "0.006670040805610944, ('mexican', 'aq0ms0')\n",
      "0.006670040805610944, ('prim', 'ao0ms0')\n",
      "0.006670478085065589, ('desarroll', 'ncms000')\n",
      "0.006670478085065589, ('grup', 'ncms000')\n",
      "0.006670478085065589, ('mayor', 'aq0cs0')\n",
      "0.0066709151211816675, ('banc', 'ncfs000')\n",
      "0.0066709151211816675, ('nuev', 'aq0fs0')\n",
      "0.0066709151211816675, ('sistem', 'ncms000')\n",
      "0.0066713519142298314, ('crecimient', 'ncms000')\n",
      "0.0066713519142298314, ('pes', 'ncms000')\n"
     ]
    }
   ],
   "source": [
    "for r in result2[:100]:\n",
    "    print(str(r[0]) + \", \" + str(r[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, en esta ocasión a diferencia de los vectores, en la entroía la palabra traslado no aparece tan cercana a la palabra *acero*, pero si lo son cable, cobre, distancia y traslado que tienen una relación con el proceso que vive el acero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
