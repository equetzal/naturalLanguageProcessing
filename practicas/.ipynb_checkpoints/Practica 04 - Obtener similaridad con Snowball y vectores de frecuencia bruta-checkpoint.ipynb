{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 4\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "## Obtener similaridad con Snowball y vectores de frecuencia bruta\n",
    "**Instrucciones:**\n",
    "    Obtener la similaridad de nuestro texto con nuestro texto derivado mediante NTLK Snowball-Stemmer, y dado el vocabulario obtenido, generar el vector de frecuencias de aparición del contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(txt):\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\treturn BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_trash(txt):\n",
    "\timport re\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in txt:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(txt):\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tans = []\n",
    "\tstp = stopwords.words()\n",
    "\tfor w in txt:\n",
    "\t\tif w not in stp:\n",
    "\t\t\tans.append(w)\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokens):\n",
    "\tvocabulary = sorted(set(tokens))\n",
    "\treturn vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(wordList, windowSize=4):\n",
    "    contexts = dict()\n",
    "    index = 0\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if word not in contexts:\n",
    "            contexts[word] = []\n",
    "        start = max(0, index - windowSize)\n",
    "        end = min(index + windowSize, len(wordList) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i != index:\n",
    "                contexts[word].append(wordList[i])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cambiaremos nuestras funciones de derivación, por la de **snowball**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_stems(tokens):\n",
    "    ss = SnowballStemmer(\"spanish\")\n",
    "    stematized_tokens = []\n",
    "    for word in tokens:\n",
    "        stematized_tokens.append(ss.stem(word))\n",
    "    return stematized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpToJson(obj, name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'w', encoding=\"utf8\") as outfile:\n",
    "        json.dump(obj, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromJson(name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'r', encoding=\"utf8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./files/e961024.htm\", \"r\", encoding=\"utf8\")\n",
    "org_txt = f.read()\n",
    "f.close()\n",
    "\n",
    "text = delete_trash(split_text(clean_html(org_txt)))\n",
    "normalized_tokens = delete_stopwords(nltk.word_tokenize(text))\n",
    "stematized_tokens = replace_with_stems(normalized_tokens)\n",
    "dumpToJson(stematized_tokens, \"stematized_tokens_snowball.json\")\n",
    "\n",
    "tokens = stematized_tokens\n",
    "\n",
    "contexts = get_contexts(tokens)\n",
    "dumpToJson(contexts, \"contexts_stematized_snowball.json\")\n",
    "\n",
    "#tokens = loadFromJson(\"stematized_tokens_snowball.json\")\n",
    "vocabulary = get_vocabulary(tokens)\n",
    "#contexts = loadFromJson(\"contexts_stematized_snowball.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "vectors = dict()\n",
    "for target, context in contexts.items():\n",
    "    cF = dict()\n",
    "    for word in context:\n",
    "        if word not in cF:\n",
    "            cF[word] = 0\n",
    "        cF[word] += 1\n",
    "    l = numpy.zeros(len(vocabulary))\n",
    "    for i in range(len(vocabulary)):\n",
    "        word = vocabulary[i]\n",
    "        if word in cF:\n",
    "            l[i] = cF[word]\n",
    "    vectors[target] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "targetWord = \"acero\"\n",
    "v1 = vectors[SnowballStemmer(\"spanish\").stem(targetWord)]\n",
    "for word in vocabulary:\n",
    "    v2 = vectors[word]\n",
    "    cosine = numpy.dot(v1, v2) / numpy.sqrt(numpy.sum(v1 ** 2)) / numpy.sqrt(numpy.sum(v2 ** 2))\n",
    "    ans.append((word, cosine))\n",
    "ans.sort(key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./p4_docs/similarity.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for par in ans:\n",
    "        f.write(F\"{par[0]} {par[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acer 0.9999999999999999\n",
      "\n",
      "traslad 0.5303300858899106\n",
      "\n",
      "cabl 0.49999999999999994\n",
      "\n",
      "microproces 0.44721359549995787\n",
      "\n",
      "tub 0.44721359549995787\n",
      "\n",
      "vaci 0.4166666666666667\n",
      "\n",
      "desmaterializ 0.37499999999999994\n",
      "\n",
      "sin 0.2558831578595795\n",
      "\n",
      "cobr 0.24999999999999997\n",
      "\n",
      "fibr 0.24999999999999997\n",
      "\n",
      "depresion 0.23717082451262841\n",
      "\n",
      "ilog 0.22360679774997894\n",
      "\n",
      "inapropi 0.22360679774997894\n",
      "\n",
      "millon 0.2188959954707709\n",
      "\n",
      "devalu 0.21650635094610965\n",
      "\n",
      "dol 0.21501831715537345\n",
      "\n",
      "grylm 0.2041241452319315\n",
      "\n",
      "paralel 0.2041241452319315\n",
      "\n",
      "tan 0.20044593143431827\n",
      "\n",
      "rentabil 0.19999999999999998\n",
      "\n",
      "alrededor 0.19867985355975656\n",
      "\n",
      "depreci 0.1952833664712358\n",
      "\n",
      "chicag 0.18569533817705183\n",
      "\n",
      "buscatel 0.17677669529663687\n",
      "\n",
      "cremi 0.17677669529663687\n",
      "\n",
      "cubr 0.17677669529663687\n",
      "\n",
      "moned 0.17677669529663687\n",
      "\n",
      "obedec 0.17677669529663687\n",
      "\n",
      "visibl 0.17677669529663687\n",
      "\n",
      "increment 0.17446914130588115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for par in ans[:30]:\n",
    "    print(F\"{par[0]} {par[1]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
