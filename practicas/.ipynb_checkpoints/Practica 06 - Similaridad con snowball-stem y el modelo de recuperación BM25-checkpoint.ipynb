{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 6\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "## Similaridad con snowball-stem y el modelo de recuperación BM25\n",
    "**Instrucciones:**\n",
    "    Con lo visto del modelos BM25, ahora supondremos que una ventana de contexto ahora es un documento, de modo que obtendremos la similaridad de cada palabra con todos los *documentos* y obtenedremos el mejor resultado para cada palabra, de modo que podremos obtener la similaridad entre una palabra y todos los documentos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import math\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(txt):\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\treturn BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteTrash(txt):\n",
    "\timport re\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in txt:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopwords(txt):\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tans = []\n",
    "\tstp = stopwords.words()\n",
    "\tfor w in txt:\n",
    "\t\tif w not in stp:\n",
    "\t\t\tans.append(w)\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary(tokens):\n",
    "\tvocabulary = sorted(set(tokens))\n",
    "\treturn vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContexts(wordList, windowSize=4):\n",
    "    contexts = dict()\n",
    "    index = 0\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if word not in contexts:\n",
    "            contexts[word] = []\n",
    "        start = max(0, index - windowSize)\n",
    "        end = min(index + windowSize, len(wordList) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i != index:\n",
    "                contexts[word].append(wordList[i])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cambiaremos nuestras funciones de derivación, por la de **snowball**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWithStems(tokens):\n",
    "    ss = SnowballStemmer(\"spanish\")\n",
    "    stematized_tokens = []\n",
    "    for word in tokens:\n",
    "        stematized_tokens.append(ss.stem(word))\n",
    "    return stematized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpToJson(obj, name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'w', encoding=\"utf8\") as outfile:\n",
    "        json.dump(obj, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromJson(name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'r', encoding=\"utf8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./files/e961024.htm\", \"r\", encoding=\"utf8\")\n",
    "org_txt = f.read()\n",
    "f.close()\n",
    "\n",
    "text = deleteTrash(splitText(cleanHtml(org_txt)))\n",
    "normalizedTokens = deleteStopwords(nltk.word_tokenize(text))\n",
    "stematizedTokens = replaceWithStems(normalizedTokens)\n",
    "tokens = stematizedTokens\n",
    "#dumpToJson(tokens, \"stematized_tokens_snowball.json\")\n",
    "\n",
    "contexts = getContexts(tokens)\n",
    "#dumpToJson(contexts, \"contexts_stematized_snowball.json\")\n",
    "\n",
    "#tokens = loadFromJson(\"stematized_tokens_snowball.json\")\n",
    "vocabulary = getVocabulary(tokens)\n",
    "#contexts = loadFromJson(\"contexts_stematized_snowball.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencyVectors(vocabulary, contexts):\n",
    "    freqVectors = dict()\n",
    "    for target in vocabulary:\n",
    "        vector = list()\n",
    "        for word in vocabulary:\n",
    "            vector.append(contexts[target].count(word))\n",
    "        freqVectors[target] = vector\n",
    "    return freqVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvg(contexts):\n",
    "    return sum(len(context) for centralWord,context in contexts.items()) / len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocs(qi, contexts):\n",
    "    total = 0\n",
    "    for centralWord,context in contexts.items():\n",
    "        if qi in context:\n",
    "            total = total+1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDF(N,q,contexts):\n",
    "    docs = getDocs(q,contexts)\n",
    "    return math.log( (N-docs+0.5)/(docs+0.5) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdfMap(vocabulary, contexts):\n",
    "    N = len(vocabulary)\n",
    "    idfMap = dict()\n",
    "    for target in vocabulary:\n",
    "        idfMap[target] = getIDF(N,target, contexts)\n",
    "    return idfMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequency(q,doc):\n",
    "    return doc.count(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(tf, mg, idf, avgdl, k , b):\n",
    "    return ((tf*(k+1)) / (tf+k*( 1-b+( (b*mg)/avgdl) ) ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCos(v1, v2):\n",
    "    n = len(v1)\n",
    "    \n",
    "    pp = 0\n",
    "    for i in range(0,n):\n",
    "        pp += (v1[i]*v2[i])\n",
    "    \n",
    "    mg1 = 0\n",
    "    mg2 = 0\n",
    "    for i in range(0,n):\n",
    "        mg1 += (v1[i]*v1[i])\n",
    "        mg2 += (v2[i]*v2[i])\n",
    "    mgV1 = math.sqrt(mg1)\n",
    "    mgV2 = math.sqrt(mg2)\n",
    "    \n",
    "    if mg1 == 0 or mg2 == 0:\n",
    "        return 0\n",
    "    return pp/(mgV1*mgV2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBM25(vocabulary, contexts):\n",
    "    k = 1.5\n",
    "    b = 0.75\n",
    "    \n",
    "    idfMap = getIdfMap(vocabulary, contexts)\n",
    "    freqVectors = getFrequencyVectors(vocabulary, contexts)\n",
    "    avg = getAvg(contexts)\n",
    "    \n",
    "    mapOfBM25 = dict()\n",
    "    for target in vocabulary:\n",
    "        fV = freqVectors[target]\n",
    "        mg = len(contexts[target])\n",
    "        \n",
    "        vector = list()\n",
    "        for freq in fV:\n",
    "            vector.append(getScore(freq,mg,idfMap[target],avg,k,b))\n",
    "        mapOfBM25[target] = vector\n",
    "    return mapOfBM25        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarsTo(target, vectors):\n",
    "    w = SnowballStemmer(\"spanish\").stem(target)\n",
    "    v1 = vectors[w]\n",
    "    \n",
    "    similarity = list()\n",
    "    for word in vocabulary:\n",
    "        v2 = vectors[word]\n",
    "        cos = getCos(v1,v2)\n",
    "        if cos > 0.0 :\n",
    "            similarity.append((cos,word))\n",
    "    \n",
    "    return sorted(similarity, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = getBM25(vocabulary, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = getSimilarsTo(\"acero\", bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002, acer\n",
      "0.5303300858899107, traslad\n",
      "0.4900443295315784, cabl\n",
      "0.4161812435586644, tub\n",
      "0.4161812435586644, microproces\n",
      "0.38253821696366713, vaci\n",
      "0.375, desmaterializ\n",
      "0.25828949695182796, ilog\n",
      "0.25, fibr\n",
      "0.22633159011550752, cobr\n",
      "0.20613200813525495, depresion\n",
      "0.1832111283478356, alrededor\n",
      "0.1767766952966369, obedec\n",
      "0.1767766952966369, cremi\n",
      "0.17305798843973946, buscatel\n",
      "0.16372083932838835, sin\n",
      "0.1634990808969134, paralel\n",
      "0.1634990808969134, grylm\n",
      "0.1578917466068364, inapropi\n",
      "0.1537519020460427, tan\n",
      "0.15322127154477425, millon\n",
      "0.1506020829576032, pes\n",
      "0.1505190248013665, penetr\n",
      "0.1505190248013665, inyeccion\n",
      "0.14890963438890767, desplom\n",
      "0.14860915849551184, visibl\n",
      "0.14860915849551184, cubr\n",
      "0.14753019676839768, injustific\n",
      "0.14749352981765051, rentabil\n",
      "0.14737547814520655, mil\n",
      "0.1467612835187306, inscinc\n",
      "0.1467612835187306, corporation\n",
      "0.14529168961955197, increment\n",
      "0.14506882256477227, finaliz\n",
      "0.14506882256477227, concesion\n",
      "0.14346993603593514, total\n",
      "0.14330768802894522, cos\n",
      "0.1411529557348831, sign\n",
      "0.1410111581572076, proporcion\n",
      "0.13702771020695884, chicag\n",
      "0.13669366484117754, moned\n",
      "0.13373116791882242, decrement\n",
      "0.13214474582404886, dcr\n",
      "0.13041863388211825, mayore\n",
      "0.13041863388211825, contien\n",
      "0.12966615483549324, captacion\n",
      "0.129144748475914, radiomex\n",
      "0.129144748475914, provision\n",
      "0.129144748475914, montacarg\n",
      "0.129144748475914, küng\n",
      "0.129144748475914, insuls\n",
      "0.129144748475914, importantisim\n",
      "0.129144748475914, empeor\n",
      "0.12914474847591398, millonews\n",
      "0.12914474847591398, endud\n",
      "0.12914474847591398, cund\n",
      "0.12914474847591398, constructor\n",
      "0.12914474847591398, aflor\n",
      "0.12800559412874848, produccion\n",
      "0.1261816982836544, año\n",
      "0.12612335769013303, viviend\n",
      "0.12595869058446965, sol\n",
      "0.1252507123696397, inferior\n",
      "0.125, yacimient\n",
      "0.125, voltaj\n",
      "0.125, volt\n",
      "0.125, vers\n",
      "0.125, verl\n",
      "0.125, veinten\n",
      "0.125, vallart\n",
      "0.125, trasform\n",
      "0.125, transistor\n",
      "0.125, tiburon\n",
      "0.125, tandr\n",
      "0.125, tabl\n",
      "0.125, suscrib\n",
      "0.125, sembr\n",
      "0.125, sardin\n",
      "0.125, romp\n",
      "0.125, riquez\n",
      "0.125, revistuch\n",
      "0.125, respald\n",
      "0.125, remanent\n",
      "0.125, reestrutur\n",
      "0.125, proletari\n",
      "0.125, primit\n",
      "0.125, pos\n",
      "0.125, polvor\n",
      "0.125, pimentel\n",
      "0.125, perdon\n",
      "0.125, pasquin\n",
      "0.125, parasit\n",
      "0.125, optic\n",
      "0.125, olas\n",
      "0.125, ocasional\n",
      "0.125, obscurec\n",
      "0.125, nihil\n",
      "0.125, nerviosim\n",
      "0.125, microeconom\n",
      "0.125, maxi\n"
     ]
    }
   ],
   "source": [
    "for r in result[:100]:\n",
    "    print(str(r[0]) + \", \" + r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el model BM25 parece ser bastante acertado a pesar de que no estamos trabajando con documentos directamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
