{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 7\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "## Similaridad  con snowball-stem y el modelo de recuperación TF-IDF\n",
    "**Instrucciones:**\n",
    "    Con lo visto del modelos TF-IDF, ahora supondremos que una ventana de contexto ahora es un documento, de modo que obtendremos la similaridad de cada palabra con todos los *documentos* y obtenedremos el mejor resultado para cada palabra, de modo que podremos obtener la similaridad entre una palabra y todos los documentos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import math\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(txt):\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\treturn BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteTrash(txt):\n",
    "\timport re\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in txt:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopwords(txt):\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tans = []\n",
    "\tstp = stopwords.words()\n",
    "\tfor w in txt:\n",
    "\t\tif w not in stp:\n",
    "\t\t\tans.append(w)\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary(tokens):\n",
    "\tvocabulary = sorted(set(tokens))\n",
    "\treturn vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContexts(wordList, windowSize=4):\n",
    "    contexts = dict()\n",
    "    index = 0\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if word not in contexts:\n",
    "            contexts[word] = []\n",
    "        start = max(0, index - windowSize)\n",
    "        end = min(index + windowSize, len(wordList) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i != index:\n",
    "                contexts[word].append(wordList[i])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWithStems(tokens):\n",
    "    ss = SnowballStemmer(\"spanish\")\n",
    "    stematized_tokens = []\n",
    "    for word in tokens:\n",
    "        stematized_tokens.append(ss.stem(word))\n",
    "    return stematized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpToJson(obj, name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'w', encoding=\"utf8\") as outfile:\n",
    "        json.dump(obj, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromJson(name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'r', encoding=\"utf8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./files/e961024.htm\", \"r\", encoding=\"utf8\")\n",
    "org_txt = f.read()\n",
    "f.close()\n",
    "\n",
    "text = deleteTrash(splitText(cleanHtml(org_txt)))\n",
    "normalizedTokens = deleteStopwords(nltk.word_tokenize(text))\n",
    "stematizedTokens = replaceWithStems(normalizedTokens)\n",
    "tokens = stematizedTokens\n",
    "#dumpToJson(tokens, \"stematized_tokens_snowball.json\")\n",
    "\n",
    "contexts = getContexts(tokens)\n",
    "#dumpToJson(contexts, \"contexts_stematized_snowball.json\")\n",
    "\n",
    "#tokens = loadFromJson(\"stematized_tokens_snowball.json\")\n",
    "vocabulary = getVocabulary(tokens)\n",
    "#contexts = loadFromJson(\"contexts_stematized_snowball.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCos(v1, v2):\n",
    "    n = len(v1)\n",
    "    \n",
    "    pp = 0\n",
    "    for i in range(0,n):\n",
    "        pp += (v1[i]*v2[i])\n",
    "    \n",
    "    mg1 = 0\n",
    "    mg2 = 0\n",
    "    for i in range(0,n):\n",
    "        mg1 += (v1[i]*v1[i])\n",
    "        mg2 += (v2[i]*v2[i])\n",
    "    mgV1 = math.sqrt(mg1)\n",
    "    mgV2 = math.sqrt(mg2)\n",
    "    \n",
    "    if mg1 == 0 or mg2 == 0:\n",
    "        return 0\n",
    "    return pp/(mgV1*mgV2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarsTo(target, vectors):\n",
    "    w = SnowballStemmer(\"spanish\").stem(target)\n",
    "    v1 = vectors[w]\n",
    "    \n",
    "    similarity = list()\n",
    "    for word in vocabulary:\n",
    "        v2 = vectors[word]\n",
    "        cos = getCos(v1,v2)\n",
    "        if cos > 0.0 :\n",
    "            similarity.append((cos,word))\n",
    "    \n",
    "    return sorted(similarity, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencyVectors(vocabulary, contexts):\n",
    "    freqVectors = dict()\n",
    "    for target in vocabulary:\n",
    "        vector = list()\n",
    "        for word in vocabulary:\n",
    "            vector.append(contexts[target].count(word))\n",
    "        freqVectors[target] = vector\n",
    "    return freqVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvg(contexts):\n",
    "    return sum(len(context) for centralWord,context in contexts.items()) / len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocs(qi, contexts):\n",
    "    total = 0\n",
    "    for centralWord,context in contexts.items():\n",
    "        if qi in context:\n",
    "            total = total+1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDF(N,q,contexts):\n",
    "    docs = getDocs(q,contexts)\n",
    "    return math.log( (N-docs+0.5)/(docs+0.5) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdfMap(vocabulary, contexts):\n",
    "    N = len(vocabulary)\n",
    "    idfMap = dict()\n",
    "    for target in vocabulary:\n",
    "        idfMap[target] = getIDF(N,target, contexts)\n",
    "    return idfMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequency(q,doc):\n",
    "    return doc.count(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(tf, mg, idf, avgdl, k , b):\n",
    "    return ((tf*(k+1)) / (tf+k*( 1-b+( (b*mg)/avgdl) ) ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Utilizaremos las mismas funciones de BM25, pero en esta ocasión con la fórmula de $tf*idf$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfIdf(vocabulary, contexts):    \n",
    "    idfMap = getIdfMap(vocabulary, contexts)\n",
    "    freqVectors = getFrequencyVectors(vocabulary, contexts)\n",
    "    \n",
    "    vectors = dict()\n",
    "    for target in vocabulary:\n",
    "        fV = freqVectors[target]\n",
    "        mg = len(contexts[target])\n",
    "        \n",
    "        vector = list()\n",
    "        for freq in fV:\n",
    "            vector.append(freq*idfMap[target])\n",
    "        vectors[target] = vector\n",
    "    return vectors        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = getTfIdf(vocabulary, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = getSimilarsTo(\"acero\", tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002, acer\n",
      "0.5303300858899106, traslad\n",
      "0.4999999999999999, cabl\n",
      "0.44721359549995804, tub\n",
      "0.44721359549995804, microproces\n",
      "0.4166666666666667, vaci\n",
      "0.3750000000000001, desmaterializ\n",
      "0.2558831578595797, sin\n",
      "0.25000000000000006, fibr\n",
      "0.25000000000000006, cobr\n",
      "0.23717082451262853, depresion\n",
      "0.22360679774997905, inapropi\n",
      "0.22360679774997902, ilog\n",
      "0.21889599547077093, millon\n",
      "0.2165063509461096, devalu\n",
      "0.21501831715537362, dol\n",
      "0.20412414523193156, grylm\n",
      "0.2041241452319315, paralel\n",
      "0.20044593143431844, tan\n",
      "0.19999999999999998, rentabil\n",
      "0.19867985355975665, alrededor\n",
      "0.1952833664712358, depreci\n",
      "0.18569533817705183, chicag\n",
      "0.17677669529663695, moned\n",
      "0.17677669529663692, buscatel\n",
      "0.1767766952966369, obedec\n",
      "0.1767766952966369, cremi\n",
      "0.17677669529663687, visibl\n",
      "0.17677669529663687, cubr\n",
      "0.17446914130588104, increment\n",
      "0.17206180040292138, desplom\n",
      "0.17160161824591105, cos\n",
      "0.1698415551216895, diciembr\n",
      "0.1666666666666667, ponent\n",
      "0.16666666666666666, subvalu\n",
      "0.16666666666666666, estabiliz\n",
      "0.16666666666666666, alcanc\n",
      "0.16549723869628927, mil\n",
      "0.16342041321085332, total\n",
      "0.1613743060919757, captacion\n",
      "0.15811388300841903, sicolog\n",
      "0.158113883008419, frijol\n",
      "0.15631561627088145, año\n",
      "0.1550434182365105, proporcion\n",
      "0.14731391274719732, dcr\n",
      "0.14708710135363806, comenz\n",
      "0.14508225945509184, pes\n",
      "0.1386750490563073, luz\n",
      "0.1386750490563073, finaliz\n",
      "0.1386750490563073, expus\n",
      "0.1386750490563073, concesion\n",
      "0.13834964763236665, gananci\n",
      "0.13538259026847055, vent\n",
      "0.13525044520011478, qued\n",
      "0.13470397652008126, sign\n",
      "0.13363062095621223, eufori\n",
      "0.1336306209562122, super\n",
      "0.13055824196677351, segun\n",
      "0.13055824196677338, norm\n",
      "0.1290994448735806, inscinc\n",
      "0.1290994448735806, corporation\n",
      "0.12909944487358058, vide\n",
      "0.12598815766974253, produccion\n",
      "0.12500000000000006, injustific\n",
      "0.12500000000000006, compromet\n",
      "0.12500000000000003, yacimient\n",
      "0.12500000000000003, voltaj\n",
      "0.12500000000000003, volt\n",
      "0.12500000000000003, vers\n",
      "0.12500000000000003, verl\n",
      "0.12500000000000003, veinten\n",
      "0.12500000000000003, vallart\n",
      "0.12500000000000003, trasform\n",
      "0.12500000000000003, transistor\n",
      "0.12500000000000003, tiburon\n",
      "0.12500000000000003, tandr\n",
      "0.12500000000000003, tabl\n",
      "0.12500000000000003, suscrib\n",
      "0.12500000000000003, spot\n",
      "0.12500000000000003, sembr\n",
      "0.12500000000000003, sardin\n",
      "0.12500000000000003, romp\n",
      "0.12500000000000003, riquez\n",
      "0.12500000000000003, revistuch\n",
      "0.12500000000000003, respald\n",
      "0.12500000000000003, resistent\n",
      "0.12500000000000003, remont\n",
      "0.12500000000000003, remanent\n",
      "0.12500000000000003, reestrutur\n",
      "0.12500000000000003, proletari\n",
      "0.12500000000000003, primit\n",
      "0.12500000000000003, presupuest\n",
      "0.12500000000000003, pos\n",
      "0.12500000000000003, portavoz\n",
      "0.12500000000000003, polvor\n",
      "0.12500000000000003, pimentel\n",
      "0.12500000000000003, perdon\n",
      "0.12500000000000003, pasquin\n",
      "0.12500000000000003, parasit\n",
      "0.12500000000000003, paramed\n"
     ]
    }
   ],
   "source": [
    "for r in result[:100]:\n",
    "    print(str(r[0]) + \", \" + r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el modelo TF-IDF nos devuelve resultados muy similares a los obtenidos con BM25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
