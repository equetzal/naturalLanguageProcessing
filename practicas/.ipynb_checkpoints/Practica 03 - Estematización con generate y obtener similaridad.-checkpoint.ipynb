{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 3\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "## Estematización con generate y obtener similaridad\n",
    "**Instrucciones:**\n",
    "    Obtener la similaridad de nuestro texto pero esta ocasión con nuestro texto derivado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(txt):\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\treturn BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_trash(txt):\n",
    "\timport re\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in txt:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(txt):\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tans = []\n",
    "\tstp = stopwords.words()\n",
    "\tfor w in txt:\n",
    "\t\tif w not in stp:\n",
    "\t\t\tans.append(w)\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokens):\n",
    "\tvocabulary = sorted(set(tokens))\n",
    "\treturn vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(wordList, windowSize=4):\n",
    "    contexts = dict()\n",
    "    index = 0\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if word not in contexts:\n",
    "            contexts[word] = []\n",
    "        start = max(0, index - windowSize)\n",
    "        end = min(index + windowSize, len(wordList) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i != index:\n",
    "                contexts[word].append(wordList[i])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stems_raw():\n",
    "    with open(\"./files/generate.txt\", 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    stems = {}\n",
    "    for line in lines:\n",
    "        if line != \"\":\n",
    "            words = [word.strip() for word in line.split()]\n",
    "            complete_word = words[0].replace(\"#\", \"\")\n",
    "            stem_word = words[0].split(\"#\")[0]\n",
    "            stems[complete_word] = stem_word\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_stems(tokens):\n",
    "    stems = loadFromJson(\"stems_dic.json\")\n",
    "    stematized_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in stems.keys():\n",
    "            stematized_tokens.append(stems[word])\n",
    "        else:\n",
    "            stematized_tokens.append(word)\n",
    "    return stematized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpToJson(obj, name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'w', encoding=\"utf8\") as outfile:\n",
    "        json.dump(obj, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromJson(name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'r', encoding=\"utf8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"./files/e961024.htm\", \"r\", encoding=\"utf8\")\n",
    "#org_txt = f.read()\n",
    "#f.close()\n",
    "\n",
    "#text = delete_trash(split_text(clean_html(org_txt)))\n",
    "#normalized_tokens = delete_stopwords(nltk.word_tokenize(text))\n",
    "#stematized_tokens = replace_with_stems(normalized_tokens)\n",
    "#dumpToJson(stematized_tokens, \"stematized_tokens_generate.json\")\n",
    "\n",
    "#contexts = get_contexts(tokens)\n",
    "#dumpToJson(contexts, \"contexts_stematized_generate.json\")\n",
    "\n",
    "tokens = loadFromJson(\"stematized_tokens_generate.json\")\n",
    "vocabulary = get_vocabulary(tokens)\n",
    "contexts = loadFromJson(\"contexts_stematized_generate.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos recalculado los contexttos de cada palabra con nuestros tokens lematizados, probaremos nuevamente la similaridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "vectors = dict()\n",
    "for target, context in contexts.items():\n",
    "    cF = dict()\n",
    "    for word in context:\n",
    "        if word not in cF:\n",
    "            cF[word] = 0\n",
    "        cF[word] += 1\n",
    "    l = numpy.zeros(len(vocabulary))\n",
    "    for i in range(len(vocabulary)):\n",
    "        word = vocabulary[i]\n",
    "        if word in cF:\n",
    "            l[i] = cF[word]\n",
    "    vectors[target] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "v1 = vectors[\"acero\"]\n",
    "for word in vocabulary:\n",
    "    v2 = vectors[word]\n",
    "    cosine = numpy.dot(v1, v2) / numpy.sqrt(numpy.sum(v1 ** 2)) / numpy.sqrt(numpy.sum(v2 ** 2))\n",
    "    ans.append((word, cosine))\n",
    "ans.sort(key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./p3_docs/similarity.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for par in ans:\n",
    "        f.write(F\"{par[0]} {par[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acero 0.9999999999999999\n",
      "\n",
      "pesado 0.7499999999999999\n",
      "\n",
      "cobre 0.5590169943749473\n",
      "\n",
      "traslad 0.5303300858899106\n",
      "\n",
      "cable 0.49999999999999994\n",
      "\n",
      "microprocesadores 0.44721359549995787\n",
      "\n",
      "tubo 0.44721359549995787\n",
      "\n",
      "vacío 0.4166666666666667\n",
      "\n",
      "desmaterializado 0.37499999999999994\n",
      "\n",
      "fibra 0.24999999999999997\n",
      "\n",
      "penetración 0.24999999999999997\n",
      "\n",
      "sostenible 0.22360679774997894\n",
      "\n",
      "sino 0.20912882830467722\n",
      "\n",
      "desplome 0.2080125735844609\n",
      "\n",
      "paralelamente 0.2041241452319315\n",
      "\n",
      "cubr 0.17677669529663687\n",
      "\n",
      "depresión 0.15811388300841894\n",
      "\n",
      "frijol 0.15811388300841894\n",
      "\n",
      "consumo 0.1436739427831727\n",
      "\n",
      "finaliz 0.1386750490563073\n",
      "\n",
      "luz 0.1386750490563073\n",
      "\n",
      "tan 0.13750477455423168\n",
      "\n",
      "alrededor 0.1348399724926484\n",
      "\n",
      "extern 0.13130643285972254\n",
      "\n",
      "producción 0.13102435641608368\n",
      "\n",
      "mitad 0.12803687993289595\n",
      "\n",
      "cosa 0.12691827064170236\n",
      "\n",
      "aburrimiento 0.12499999999999999\n",
      "\n",
      "acos 0.12499999999999999\n",
      "\n",
      "adepto 0.12499999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for par in ans[:30]:\n",
    "    print(F\"{par[0]} {par[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, ahora la palabra **cobre** paso de tener *0.49* de similitud a *0.55*, lo cual fué sufienciente para pasar a ser la segunda palabra más similar a **acero**, por lo que podemos notar una mejora en nuestro programa de similitud gracias a la derivación, y con resultados muy similares a los obtenidos en la lematización. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
