{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 12\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "## Información Mutua con KL-Divergence\n",
    "**Instrucciones:**\n",
    "    Utilizar la incertidumbre de información mutua cominado con entropía, utilizando la formula de divergencia de Kullback-Liebler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import math\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(txt):\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\treturn BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteTrash(txt):\n",
    "\timport re\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in txt:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitToSentences(txt):\n",
    "    tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "    return tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopwords(txt):\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tans = []\n",
    "\tstp = stopwords.words()\n",
    "\tfor w in txt:\n",
    "\t\tif w not in stp:\n",
    "\t\t\tans.append(w)\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary(sentences):\n",
    "    tokens = set()\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens.add(token)\n",
    "    return sorted(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencies(sentences, vocabulary):\n",
    "    tokens = dict()\n",
    "    \n",
    "    for token in vocabulary:\n",
    "        tokens[token] = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens[token] += 1\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContexts(wordList, windowSize=4):\n",
    "    contexts = dict()\n",
    "    index = 0\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if word not in contexts:\n",
    "            contexts[word] = []\n",
    "        start = max(0, index - windowSize)\n",
    "        end = min(index + windowSize, len(wordList) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i != index:\n",
    "                contexts[word].append(wordList[i])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromJson(name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'r', encoding=\"utf8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpToJson(obj, name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'w', encoding=\"utf8\") as outfile:\n",
    "        json.dump(obj, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWithStems(tokens):\n",
    "    ss = SnowballStemmer(\"spanish\")\n",
    "    stematized_tokens = []\n",
    "    for word in tokens:\n",
    "        stematized_tokens.append(ss.stem(word))\n",
    "    return stematized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacWithLemmas(tokens):\n",
    "    lemmas = loadFromJson(\"lemmatized_tokens_generate\")\n",
    "    lemmatized_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_tokens.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_tokens.append(word)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTagger():\n",
    "    patterns = [\n",
    "        (r'.*o$', 'n'),  # noun masculine singular\n",
    "        (r'.*os$', 'n'), # noun masculine plural\n",
    "        (r'.*a$', 'n'),  # noun femenine singular\n",
    "        (r'.*as$', 'n')  # noun femenine plural\n",
    "    ]\n",
    "    regexTagger = nltk.RegexpTagger(patterns, nltk.DefaultTagger('s'))\n",
    "    unigramTagger = nltk.UnigramTagger(nltk.corpus.cess_esp.tagged_sents(), None, regexTagger)\n",
    "    return unigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixTags(tokens, tokenTags):\n",
    "    taggedTokens = list()\n",
    "    for i in range(0,len(tokens)):\n",
    "        taggedTokens.append((tokens[i], tokenTags[i][1]))\n",
    "    return taggedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeSencences(sent):\n",
    "    tagger = getTagger()\n",
    "    sentences = []\n",
    "    for s in sent:\n",
    "        cleanSentence = deleteTrash(splitText(s))\n",
    "        normalizedTokens = deleteStopwords(nltk.word_tokenize(cleanSentence))\n",
    "        tokenTags = tagger.tag(normalizedTokens)\n",
    "        stematizedTokens = replaceWithStems(normalizedTokens)\n",
    "        tokens = mixTags(stematizedTokens, tokenTags)\n",
    "        sentences.append(tokens)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./files/e961024.htm\", \"r\", encoding=\"utf8\")\n",
    "org_txt = f.read()\n",
    "f.close()\n",
    "\n",
    "sentences = splitToSentences(cleanHtml(org_txt))\n",
    "sentences = normalizeSencences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = getVocabulary(sentences)\n",
    "frequencies = getFrequencies(sentences, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando la Entropía\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntropy(tgP, wordP, target, word, n, sentences):\n",
    "    prob_11 = 0\n",
    "    for sent in sentences:\n",
    "        if (target in sent) and (word in sent):\n",
    "             prob_11 += 1\n",
    "    prob_11 /= n\n",
    "    \n",
    "    tgQ = 1 - tgP\n",
    "    wordQ = 1 - wordP\n",
    "    \n",
    "    prob_01 = wordP - prob_11\n",
    "    prob_10 = tgP - prob_11\n",
    "    prob_00 = tgQ - prob_01\n",
    "    \n",
    "    #print(\"prob => \",prob_00, prob_01, prob_10, prob_11)\n",
    "    \n",
    "    a,b,c,d = 0,0,0,0\n",
    "    if wordQ != 0:\n",
    "        param = prob_00/(tgQ*wordQ) #KL divergence\n",
    "        if param > 0:\n",
    "            a = prob_00 * math.log2(param)\n",
    "        param = prob_10/(tgP*wordQ)\n",
    "        if param > 0:\n",
    "            b = prob_10 * math.log2(param)\n",
    "            \n",
    "    if wordP != 0:\n",
    "        param = prob_01/(tgQ*wordP)\n",
    "        if param > 0:\n",
    "            c = prob_01 * math.log2(param)\n",
    "        param = prob_11/(tgP*wordP)\n",
    "        if param > 0:\n",
    "            d = prob_11 * math.log2(param)\n",
    "            \n",
    "    #print(\"entropy => \",a,b,c,d)\n",
    "            \n",
    "    return (a+b+c+d)*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarsTo(target, frequency, vocabulary, sentences):\n",
    "    w = SnowballStemmer(\"spanish\").stem(target)\n",
    "    n = len(sentences)\n",
    "    k = 2 #Laplace Smoothing variable\n",
    "    \n",
    "    targetTokens = set()\n",
    "    for token in vocabulary:\n",
    "        if token[0] == w:\n",
    "            targetTokens.add(token)\n",
    "    \n",
    "    print(\"Checking for \" + str(len(targetTokens)) + \" tokens of word \" + target)\n",
    "    for tg in targetTokens:\n",
    "        print(\"\\t\" + str(tg))\n",
    "            \n",
    "    similarity = dict()\n",
    "    for word in vocabulary:\n",
    "        similarity[word] = 0.0\n",
    "    for tg in targetTokens:\n",
    "        tgProb = (frequency[tg]+k)/(n+frequency[tg]*k)\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            wProb = (frequency[word]+k)/(n+frequency[tg]*k)\n",
    "            entropy = getEntropy(tgProb, wProb, tg, word, n, sentences)\n",
    "            similarity[word] += entropy\n",
    "    \n",
    "    relatedWords = []\n",
    "    for token,entropy in similarity.items():\n",
    "        relatedWords.append((entropy,token))\n",
    "    \n",
    "    return sorted(relatedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 8 tokens of word economía\n",
      "\t('econom', 'aq0fp0')\n",
      "\t('econom', 'ncfp000')\n",
      "\t('econom', 'nccp000')\n",
      "\t('econom', 'aq0fs0')\n",
      "\t('econom', 'aq0mp0')\n",
      "\t('econom', 'aq0ms0')\n",
      "\t('econom', 'nccs000')\n",
      "\t('econom', 'ncfs000')\n"
     ]
    }
   ],
   "source": [
    "result = getSimilarsTo(\"economía\", frequencies, vocabulary, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.12215123286222171, ('econom', 'ncfs000')\n",
      "-0.1179139460041253, ('econom', 'aq0fs0')\n",
      "-0.08817665887884213, ('econom', 'aq0ms0')\n",
      "-0.033279448289898345, ('econom', 'aq0mp0')\n",
      "-0.031486875130163836, ('econom', 'aq0fp0')\n",
      "-0.025920987920871157, ('econom', 'nccs000')\n",
      "-0.01593333095700152, ('econom', 'nccp000')\n",
      "-0.015043063137908553, ('ven', 'vmip3p0')\n",
      "-0.013711532139178298, ('recuper', 'ncfs000')\n",
      "-0.013313680359125337, ('crecimient', 'ncms000')\n",
      "-0.009548332426989405, ('econom', 'ncfp000')\n",
      "-0.009123454957945268, ('globaliz', 'ncfs000')\n",
      "-0.009037127138095263, ('polit', 'ncfs000')\n",
      "-0.008850955015992245, ('pais', 'ncms000')\n",
      "-0.008819658373953331, ('funcionari', 'ncmp000')\n",
      "-0.008696320680427673, ('mundial', 'aq0cs0')\n",
      "-0.008687966293415988, ('financier', 'aq0mp0')\n",
      "-0.008473166257941662, ('perversion', 's')\n",
      "-0.008275545766920224, ('conoc', 'ncms000')\n",
      "-0.008052956408488663, ('crisis', 'ncfn000')\n",
      "-0.008003689688639775, ('bm', 's')\n",
      "-0.007997081386527978, ('mexican', 'aq0fs0')\n",
      "-0.007896544587901339, ('polit', 'aq0fp0')\n",
      "-0.0076133510855228865, ('banc', 'ncms000')\n",
      "-0.007543302005668935, ('error', 'ncms000')\n",
      "-0.007511926073775354, ('internacional', 'aq0cs0')\n",
      "-0.007495492770086605, ('product', 'ncmp000')\n",
      "-0.0073749456901289695, ('centr', 'ncms000')\n",
      "-0.007333915224563886, ('not', 'ncfs000')\n",
      "-0.007318669177306351, ('educ', 'ncfs000')\n",
      "-0.0072271290083273645, ('vez', 'ncfs000')\n",
      "-0.007226406671080632, ('requier', 'vmip3s0')\n",
      "-0.0071334015447064416, ('siguient', 'aq0cs0')\n",
      "-0.007116742572737737, ('sin', 'cc')\n",
      "-0.007036183897022588, ('html', 's')\n",
      "-0.007036183897022588, ('http', 's')\n",
      "-0.007036183897022588, ('mx', 's')\n",
      "-0.007036183897022588, ('www', 's')\n",
      "-0.0069131180967429385, ('public', 'aq0ms0')\n",
      "-0.0068199056830541525, ('bas', 'aq0fsp')\n",
      "-0.006806545810984413, ('autor', 'ncms000')\n",
      "-0.006781531654652211, ('porcentaj', 'ncms000')\n",
      "-0.006737120595052927, ('model', 'ncms000')\n",
      "-0.006709036205263197, ('confianz', 'ncfs000')\n",
      "-0.006699713501613427, ('estabil', 'ncfs000')\n",
      "-0.00669742687595409, ('cas', 'ncms000')\n",
      "-0.006696427440910465, ('nuev', 'aq0fs0')\n",
      "-0.006675867676798306, ('pes', 'ncms000')\n",
      "-0.006666381651846595, ('juev', 'W')\n",
      "-0.006658228066180552, ('import', 'aq0cs0')\n",
      "-0.006614803005317765, ('asi', 'rg')\n",
      "-0.006500016493297435, ('desemple', 'ncms000')\n",
      "-0.006387609593891222, ('cad', 'di0cs0')\n",
      "-0.006384169610058705, ('macroeconom', 'n')\n",
      "-0.006340200613831029, ('ct', 's')\n",
      "-0.0062779699702788395, ('public', 'aq0mp0')\n",
      "-0.006152263232610425, ('perspect', 'ncfs000')\n",
      "-0.006136974864280609, ('ingres', 'ncmp000')\n",
      "-0.0061173968062709465, ('mal', 'aq0mp0')\n",
      "-0.0061173968062709465, ('mit', 'ncmp000')\n",
      "-0.0061173968062709465, ('paz', 'n')\n",
      "-0.0061173968062709465, ('valor', 'vmip3p0')\n",
      "-0.006029595168781442, ('eugeni', 'n')\n",
      "-0.006029595168781442, ('kuztnetsov', 's')\n",
      "-0.005926073869347305, ('institut', 'ncms000')\n",
      "-0.005921459072425568, ('telecomun', 'ncfp000')\n",
      "-0.00590468254147073, ('octubr', 'W')\n",
      "-0.005901669112215153, ('proyect', 'ncms000')\n",
      "-0.005881322418182512, ('salari', 'ncmp000')\n",
      "-0.005867297149297687, ('diferent', 'ncfs000')\n",
      "-0.005832307894263944, ('actual', 'aq0cs0')\n",
      "-0.005802344691640026, ('efect', 'ncmp000')\n",
      "-0.005697009348544591, ('residual', 'aq0cp0')\n",
      "-0.005694672496981145, ('estrategi', 'ncfs000')\n",
      "-0.0056356032720739505, ('visibl', 'aq0cp0')\n",
      "-0.005599165455700319, ('añad', 'vmis3s0')\n",
      "-0.005590325248103922, ('financ', 'aq0fs0')\n",
      "-0.0055493205240543225, ('bursatil', 'aq0cs0')\n",
      "-0.005456218878381934, ('segund', 'ao0fs0')\n",
      "-0.005410965245695216, ('separ', 'vmp00sm')\n",
      "-0.00540659253899042, ('gran', 'aq0cs0')\n",
      "-0.005367096724104638, ('situacion', 'ncfs000')\n",
      "-0.005354163023310979, ('lueg', 'rg')\n",
      "-0.005348181177618265, ('condicion', 'ncfp000')\n",
      "-0.005347955470627704, ('comerci', 'ncms000')\n",
      "-0.005338321699034715, ('logr', 's')\n",
      "-0.0053243766538392965, ('impuest', 'ncmp000')\n",
      "-0.005317917211181564, ('zedill', 'n')\n",
      "-0.005267570567628843, ('agu', 'ncfp000')\n",
      "-0.005267570567628843, ('refir', 'vmis3s0')\n",
      "-0.005251387800178176, ('carreon', 's')\n",
      "-0.005251387800178176, ('depend', 'vmip3p0')\n",
      "-0.005251387800178176, ('desarroll', 'aq0fpp')\n",
      "-0.00523766189327211, ('posibl', 'aq0cp0')\n",
      "-0.005229921707632781, ('gravisim', 'n')\n",
      "-0.005176302894489045, ('acontec', 's')\n",
      "-0.005161701199366334, ('mayor', 'aq0cp0')\n",
      "-0.005155391748403603, ('entorn', 'ncms000')\n",
      "-0.005091937154405094, ('ver', 'vmn0000')\n",
      "-0.005082343796386259, ('sistem', 'ncms000')\n"
     ]
    }
   ],
   "source": [
    "for r in result[:100]:\n",
    "    print(str(r[0]) + \", \" + str(r[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 1 tokens of word acero\n",
      "\t('acer', 'ncms000')\n"
     ]
    }
   ],
   "source": [
    "result2 = getSimilarsTo(\"acero\", frequencies, vocabulary, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0036922259662763987, ('acer', 'ncms000')\n",
      "-0.0036922259662763987, ('barat', 'aq0fs0')\n",
      "-0.0036922259662763987, ('cabl', 's')\n",
      "-0.0036922259662763987, ('cobr', 'ncms000')\n",
      "-0.0036922259662763987, ('codif', 's')\n",
      "-0.0036922259662763987, ('desmaterializ', 'n')\n",
      "-0.0036922259662763987, ('distanci', 'ncfp000')\n",
      "-0.0036922259662763987, ('fibr', 'n')\n",
      "-0.0036922259662763987, ('impuls', 'vmp00sm')\n",
      "-0.0036922259662763987, ('larg', 'aq0fp0')\n",
      "-0.0036922259662763987, ('microproces', 's')\n",
      "-0.0036922259662763987, ('mov', 'vmn0000')\n",
      "-0.0036922259662763987, ('noved', 's')\n",
      "-0.0036922259662763987, ('optic', 'n')\n",
      "-0.0036922259662763987, ('pes', 'vmp00sm')\n",
      "-0.0036922259662763987, ('rasg', 'n')\n",
      "-0.0036922259662763987, ('transistor', 's')\n",
      "-0.0036922259662763987, ('traslad', 'vmp00sm')\n",
      "-0.0036922259662763987, ('tub', 'ncmp000')\n",
      "-0.00342616288109798, ('aceler', 'n')\n",
      "-0.00342616288109798, ('cabl', 'ncms000')\n",
      "-0.00342616288109798, ('digital', 'aq0cs0')\n",
      "-0.00342616288109798, ('vaci', 'ncms000')\n",
      "-0.0032285516449949386, ('transmision', 'ncfs000')\n",
      "-0.0029405920800336977, ('tecnolog', 'aq0ms0')\n",
      "-0.002828861348832208, ('bas', 'aq0fsp')\n",
      "-0.002828861348832208, ('intens', 'aq0fs0')\n",
      "-0.002731284984800229, ('diferent', 'ncfs000')\n",
      "-0.002731284984800229, ('facil', 'aq0cs0')\n",
      "-0.0026446951058127968, ('industrial', 'aq0cs0')\n",
      "-0.0024962423652183045, ('epoc', 'ncfs000')\n",
      "-0.0023719615469800746, ('esta', 'pd0fs000')\n",
      "-0.0022651374826058515, ('conoc', 'ncms000')\n",
      "-0.0022168800605942996, ('cos', 'ncfp000')\n",
      "-0.002128723233566791, ('actual', 'aq0cs0')\n",
      "-0.0019451182986165326, ('siempr', 'rg')\n",
      "-0.0018826916521781393, ('econom', 'aq0ms0')\n",
      "-0.0018826916521781393, ('respect', 'ncms000')\n",
      "-0.0017979267623150635, ('sid', 'vsp00sm')\n",
      "-0.0017463905921389757, ('bien', 'rg')\n",
      "-0.0017219198083204318, ('gran', 'aq0cs0')\n",
      "-0.00167530799693525, ('produccion', 'ncfs000')\n",
      "-0.0016530751499985083, ('econom', 'ncfs000')\n",
      "-0.0015703987991221315, ('hac', 'vmn0000')\n",
      "-0.0014788633518081815, ('ahor', 'rg')\n",
      "-0.0014132734377739055, ('asi', 'rg')\n",
      "-0.0014132734377739055, ('cambi', 'ncms000')\n",
      "-0.001010656498992523, ('sol', 'rg')\n",
      "-0.00025402204021581615, ('mexic', 'np0000l')\n",
      "-0.00021861974978404234, ('millon', 'ncmp000')\n"
     ]
    }
   ],
   "source": [
    "for r in result2[:50]:\n",
    "    print(str(r[0]) + \", \" + str(r[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
