{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "## Lematización con etiquetas y obtener similaridad\n",
    "**Instrucciones:**\n",
    "    Obtener la similaridad de nuestro texto pero esta ocasión con nuestro texto lematizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(txt):\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\treturn BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_trash(txt):\n",
    "\timport re\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in txt:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(txt):\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tans = []\n",
    "\tstp = stopwords.words()\n",
    "\tfor w in txt:\n",
    "\t\tif w not in stp:\n",
    "\t\t\tans.append(w)\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokens):\n",
    "\tvocabulary = sorted(set(tokens))\n",
    "\treturn vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(wordList, windowSize=4):\n",
    "    contexts = dict()\n",
    "    index = 0\n",
    "    for index in range(len(wordList)):\n",
    "        word = wordList[index]\n",
    "        if word not in contexts:\n",
    "            contexts[word] = []\n",
    "        start = max(0, index - windowSize)\n",
    "        end = min(index + windowSize, len(wordList) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i != index:\n",
    "                contexts[word].append(wordList[i])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lemmas_raw():\n",
    "    with open(\"./files/lemmas.txt\", 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    lemmas = {}\n",
    "    for line in lines:\n",
    "        if line != \"\":\n",
    "            words = [word.strip() for word in line.split()]\n",
    "            wordform = words[0].replace(\"#\", \"\")\n",
    "            lemmas[wordform] = words[-1]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_lemmas(tokens):\n",
    "    lemmas = loadFromJson(\"lemmas_dic.json\")\n",
    "    lemmatized_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_tokens.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_tokens.append(word)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpToJson(obj, name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'w', encoding=\"utf8\") as outfile:\n",
    "        json.dump(obj, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromJson(name):\n",
    "    import json\n",
    "    with open(\"./files/\"+name, 'r', encoding=\"utf8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = loadFromJson(\"generate_lemmatized_tokens.json\")\n",
    "vocabulary = get_vocabulary(tokens)\n",
    "contexts = loadFromJson(\"contexts_generate_lemmas.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos recalculado los contexttos de cada palabra con nuestros tokens lematizados, probaremos nuevamente la similaridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "vectors = dict()\n",
    "for target, context in contexts.items():\n",
    "    cF = dict()\n",
    "    for word in context:\n",
    "        if word not in cF:\n",
    "            cF[word] = 0\n",
    "        cF[word] += 1\n",
    "    l = numpy.zeros(len(vocabulary))\n",
    "    for i in range(len(vocabulary)):\n",
    "        word = vocabulary[i]\n",
    "        if word in cF:\n",
    "            l[i] = cF[word]\n",
    "    vectors[target] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "v1 = vectors[\"acero\"]\n",
    "for word in vocabulary:\n",
    "    v2 = vectors[word]\n",
    "    cosine = numpy.dot(v1, v2) / numpy.sqrt(numpy.sum(v1 ** 2)) / numpy.sqrt(numpy.sum(v2 ** 2))\n",
    "    ans.append((word, cosine))\n",
    "ans.sort(key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./p2_docs/similarity.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for par in ans:\n",
    "        f.write(F\"{par[0]} {par[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acero 0.9999999999999999\n",
      "\n",
      "pesado 0.7499999999999999\n",
      "\n",
      "cobre 0.5590169943749473\n",
      "\n",
      "trasladar 0.5303300858899106\n",
      "\n",
      "cable 0.49999999999999994\n",
      "\n",
      "microprocesadores 0.44721359549995787\n",
      "\n",
      "tubo 0.44721359549995787\n",
      "\n",
      "vacío 0.4166666666666667\n",
      "\n",
      "desmaterializado 0.37499999999999994\n",
      "\n",
      "fibra 0.24999999999999997\n",
      "\n",
      "penetración 0.24999999999999997\n",
      "\n",
      "sostenible 0.22360679774997894\n",
      "\n",
      "sino 0.20887610455277092\n",
      "\n",
      "desplome 0.2080125735844609\n",
      "\n",
      "paralelamente 0.2041241452319315\n",
      "\n",
      "cubrir 0.1599005372667078\n",
      "\n",
      "depresión 0.15811388300841894\n",
      "\n",
      "frijol 0.15811388300841894\n",
      "\n",
      "consumo 0.1436739427831727\n",
      "\n",
      "finalizar 0.1386750490563073\n",
      "\n",
      "luz 0.1386750490563073\n",
      "\n",
      "tan 0.1358036190637778\n",
      "\n",
      "alrededor 0.13245323570650436\n",
      "\n",
      "externo 0.13130643285972254\n",
      "\n",
      "producción 0.12830005981991682\n",
      "\n",
      "mitad 0.12803687993289595\n",
      "\n",
      "abocar 0.12499999999999999\n",
      "\n",
      "aburrimiento 0.12499999999999999\n",
      "\n",
      "acosar 0.12499999999999999\n",
      "\n",
      "adepto 0.12499999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for par in ans[:30]:\n",
    "    print(F\"{par[0]} {par[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, ahora la palabra **cobre** paso de tener *0.49* de similitud a *0.55*, lo cual fué sufienciente para pasar a ser la segunda palabra más similar a **acero**, por lo que podemos notar una mejora en nuestro programa de similitud gracias a la lematización. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
