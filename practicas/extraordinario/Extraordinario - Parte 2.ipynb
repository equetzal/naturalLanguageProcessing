{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraordinario (Parte 2)\n",
    "- **Alumna:** Enya Quetzalli Gómez Rodríguez *(Eduardo Gómez Rodríguez)*\n",
    "- **Profesora:** Olga Kolesnikova\n",
    "- **Escuela:** Escuela Superior de Cómputo del IPN\n",
    "- **Grupo:** 3CV9\n",
    "- **Semestre:** 2020/2\n",
    "\n",
    "**Instrucciones:**\n",
    "   - Analizar las reseñas de móviles y obtener aspectos importantes\n",
    "   - Realizar análisis de polaridad sobre 5 aspectos\n",
    "   - Generar un resúmen de las opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import xml.dom.minidom\n",
    "\n",
    "from gensim import corpora\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(txt):\n",
    "    from bs4 import BeautifulSoup\n",
    "    return BeautifulSoup(txt,'lxml').get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(txt):\n",
    "    return txt.replace('/', ' ').replace('.', ' ').replace('-', ' ').replace('\\n', ' ').replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteTrash(txt):\n",
    "    import re\n",
    "    good = {'\\n'}\n",
    "    for i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "        good.add(i)\n",
    "    ans = \"\"\n",
    "    for c in txt:\n",
    "        if c in good:\n",
    "            ans += c\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitToSentences(txt):\n",
    "    tokenizer = nltk.data.load('nltk:tokenizers/punkt/spanish.pickle')\n",
    "    return tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopwords(txt):\n",
    "    from nltk.corpus import stopwords\n",
    "    ans = []\n",
    "    stp = stopwords.words('spanish')\n",
    "    for w in txt:\n",
    "        if w not in stp:\n",
    "            ans.append(w)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary(sentences):\n",
    "    tokens = set()\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens.add(token)\n",
    "    return sorted(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencies(sentences, vocabulary):\n",
    "    tokens = dict()\n",
    "    \n",
    "    for token in vocabulary:\n",
    "        tokens[token] = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens[token] += 1\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWithLemmas(tokens):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word in tokens:\n",
    "        lemmatized_tokens.append(lemma.lemmatize(word))\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTagger():\n",
    "    patterns = [\n",
    "        (r'.*o$', 'n'),  # noun masculine singular\n",
    "        (r'.*os$', 'n'), # noun masculine plural\n",
    "        (r'.*a$', 'n'),  # noun femenine singular\n",
    "        (r'.*as$', 'n')  # noun femenine plural\n",
    "    ]\n",
    "    regexTagger = nltk.RegexpTagger(patterns, nltk.DefaultTagger('s'))\n",
    "    unigramTagger = nltk.UnigramTagger(nltk.corpus.cess_esp.tagged_sents(), None, regexTagger)\n",
    "    return unigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixTags(tokens, tokenTags):\n",
    "    taggedTokens = list()\n",
    "    for i in range(0,len(tokens)):\n",
    "        taggedTokens.append((tokens[i], tokenTags[i][1]))\n",
    "    return taggedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeSencences(sent):\n",
    "    tagger = getTagger()\n",
    "    sentences = []\n",
    "    for s in sent:\n",
    "        cleanSentence = deleteTrash(cleanText(s))\n",
    "        normalizedTokens = deleteStopwords(nltk.word_tokenize(cleanSentence))\n",
    "        tokenTags = tagger.tag(normalizedTokens)\n",
    "        stematizedTokens = replaceWithLemmas(normalizedTokens)\n",
    "        tokens = mixTags(stematizedTokens, tokenTags)\n",
    "        sentences.append(tokens)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeSencencesSpacy(sent):\n",
    "    tagger = getTagger()\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    sentences = []\n",
    "    \n",
    "    for s in sent:\n",
    "        cleanSentence = deleteTrash(cleanText(s))\n",
    "        normalizedTokens = nlp(cleanSentence)\n",
    "        lemmatizedTokens = [token.lemma_ for token in normalizedTokens if not token.lemma_.isspace()]\n",
    "        noStopwordsTokens = deleteStopwords(lemmatizedTokens)\n",
    "        tokenTags = tagger.tag(noStopwordsTokens)\n",
    "        tokens = mixTags(lemmatizedTokens, tokenTags)\n",
    "        sentences.append(tokens)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadText():\n",
    "    folder = \"./extraordinario/movil\"\n",
    "    lines = list()\n",
    "    for file in os.listdir(folder):\n",
    "        with open(os.path.join(folder, file)) as f:\n",
    "            lines.append(f.read())\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_txt = loadText()\n",
    "sentences = splitToSentences(org_txt)\n",
    "sentences = normalizeSencences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = getVocabulary(sentences)\n",
    "frequencies = getFrequencies(sentences, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener Aspectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for token, count in frequencies.items():\n",
    "    if token[1][0] == 'n':\n",
    "        nouns.append((count, token[0]))\n",
    "nouns.sort(reverse=True)\n",
    "aspectos = [noun[1] for noun in nouns[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma\n",
      "telefono\n",
      "bateria\n",
      "dia\n",
      "okia\n",
      "pantalla\n",
      "fotos\n",
      "n\n",
      "camara\n",
      "calidad\n",
      "memoria\n",
      "tiempo\n",
      "vez\n",
      "habia\n",
      "manos\n",
      "radio\n",
      "precio\n",
      "tienda\n",
      "color\n",
      "tipo\n"
     ]
    }
   ],
   "source": [
    "for aspecto in aspectos:\n",
    "    print(aspecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspectos = ['bateria', 'pantalla', 'fotos', 'calidad', 'memoria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPolarities(file):\n",
    "    ans = {}\n",
    "    doc = xml.dom.minidom.parse(file)\n",
    "    layers = doc.getElementsByTagName(\"senticon\")[0].getElementsByTagName(\"layer\")\n",
    "    for layer in layers:\n",
    "        positive = layer.getElementsByTagName(\"positive\")[0].getElementsByTagName(\"lemma\")\n",
    "        negative = layer.getElementsByTagName(\"negative\")[0].getElementsByTagName(\"lemma\")\n",
    "        for lemma in positive + negative:\n",
    "            ans[lemma.firstChild.nodeValue.rstrip().lstrip().lower().replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('ñ', 'n').replace('ü', 'u')] = float(lemma.getAttribute(\"pol\"))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities = getPolarities('./extraordinario/sent/senticon.es.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countAspect(aspect, sentence):\n",
    "    count = 0\n",
    "    for token in sentence:\n",
    "        if token[0] == aspect:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [[] for i in range(len(aspectos))]\n",
    "for i in range(len(aspectos)):\n",
    "    aspecto = aspectos[i]\n",
    "    for sentence in sentences:\n",
    "        if countAspect(aspecto,sentence):\n",
    "            context[i].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = np.zeros(len(aspectos))\n",
    "cnt = np.zeros(len(aspectos))\n",
    "for i in range(len(aspectos)):\n",
    "    for sentence in context[i]:\n",
    "        for word in sentence:\n",
    "            if word[0] in polarities:\n",
    "                avg[i] += polarities[word[0]]\n",
    "                cnt[i] += 1\n",
    "avg /= cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspecto    Probabildiad Promedio\n",
      "bateria    0.17302145922746787\n",
      "pantalla   0.24455952380952384\n",
      "fotos      0.3049433962264151\n",
      "calidad    0.23509271523178804\n",
      "memoria    0.25182000000000004\n"
     ]
    }
   ],
   "source": [
    "print(F\"{'Aspecto':{11}}Probabildiad Promedio\")\n",
    "for i in range(len(aspectos)):\n",
    "    print(F\"{aspectos[i]:{11}}{avg[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiments(sentiments, file):\n",
    "    with open(file) as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.rstrip('\\n').split('\\t')\n",
    "            sentiment = line[0]\n",
    "            tag = line[-1]\n",
    "            sentiments[sentiment] = (1 if tag == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = dict()\n",
    "getSentiments(sentiments, './extraordinario/sent/fullStrengthLexicon.txt')\n",
    "getSentiments(sentiments, './extraordinario/sent/mediumStrengthLexicon.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
