
# Clase Online #1

**Fecha:** 20.marzo.2020

## Actividad Online 1

Ver los videos de las clases impartidas por [Andrew Ng](https://es.wikipedia.org/wiki/Andrew_Ng)

Los videos est√°n [en esta lista](https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN).
 
Por favor, hasta ma√±ana estudian [Lecture 1.1](https://youtu.be/PPLop4L2eGk), [Lecture 1.2](https://youtu.be/bQI5uDxrFfA), [Lecture 1.3](https://youtu.be/jAA2g9ItoAc)


## Teor√≠a

*¬øC√≥mo la m√°quina aprende?*

Le damos datos, tambi√©n se llaman ejemplos. Cada ejemplo es un vector num√©rico que representa el dato correspondiente. Para im√°genes, el vector va a contener los valores de intensidades de colores de pixeles, para nosotros son caracter√≠sticas que estudiamos: frecuencia bruta, frecuencia relativa (probabilidad), tf-idf, etc. de palabras. 

Los vectores de ejemplos se representan como $x^{(i)}$ donde $i$ es el n√∫mero del ejemplo. Los elementos (features) del vector tambi√©n se representan por $x$, pero por $x$ con sub√≠ndice, por ejemplo, $x_j$ donde $j$ es el n√∫mero del elemento del vector.

Entonces cada dato es el vector de este tipo:
$x^{(i)} = (x_1, x_2, \dots, x_j, \dots, x_n)$, para toda $1\leq j \leq n$

$m$ es el n√∫mero de ejemplos y $n$ es el n√∫mero de caracter√≠sticas en un vector (longitud del vector).

Cada vector $x^{(i)}$, est√° asociado con un valor que se representa por $y^{(i)}$. Entonces el conjunto de datos del tama√±o $m$, va a contener los pares de tipo $(x^{(i)},y^{(i)})$.

La idea de aprendizaje es ense√±ar a la m√°quina estos ejemplos (pares de tipo $(x^{(i)},y^{(i)})$, para que halla aprenda a predecir el valor de $y^{(i)}$ a partir del vector $x^{(i)}$.

*¬øComo precisamente se realiza este aprendizaje?*

La m√°quina toma un vector $x^{(i)}$ y calcula el valor de $y$ que le corresponde a $x^{(i)}$ usando una cierta funci√≥n o proceso. Este valor calculado de $y$ se llama el valor estimado de $y$, y se
representa por $\hat{y}^{(i)}$. Este valor $\hat{y}^{(i)}$ normalmente es diferente del valor real $y^{(i)}$, es decir, hay un error de estimaci√≥n. El error se puede calcular usando el m√©todo de diferencias cuadradas:
$$
errorDeEstimaci√≥n = (\hat{y}^{(i)}-y^{(i)})^2
$$

El error de estimaci√≥n o predicci√≥n tambi√©n se llama **costo**. Para cada ejemplo se calcula el error
de predicci√≥n, luego se suman todos los errores y se promedian por el n√∫mero ùëö de ejemplos.
Este promedio se llama **funci√≥n de costo** que se denomina por la letra $J$
$$
J = \dfrac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)}-y^{(i)})^2
$$

La funci√≥n de costo a veces se llama funci√≥n de p√©rdida o funci√≥n de error.
Ahora lo que hay que hacer es optimizar la funci√≥n de costo, es decir, minimizar el error de predicci√≥n. El proceso de optimizaci√≥n se llama aprendizaje.

## Actividad Online 2
Por favor vean los videos:
- [Lecture 2.1](https://youtu.be/kHwlB_j7Hkc)
- [Lecture 2.2](https://youtu.be/yuH4iRcggMw) 
- [Lecture 2.3](https://youtu.be/yR2ipCoFvNo)
- [Lecture 2.4](https://youtu.be/0kns1gXLYg4)

Estudien bien su contenido, en estas clases ustedes van a conocer como se calcula el valor estimado de $\hat{y}^{(i)}$
